{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Defining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRATING = 600ZD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing GitHUB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np \n",
    "from astropy.io import fits \n",
    "from smooth_kevin import smoother\n",
    "import py_specrebin\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import rc\n",
    "import py_specrebin\n",
    "import pandas as pd\n",
    "path_name = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caution: Execute the following cell only once per run. Do not modify the ```std_out``` or ```std_err``` variables. If they are modified by accident, please restart the kernel and run the notebook from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the original streams for stdout and stderr. To be used for logging output later\n",
    "import sys\n",
    "std_out = sys.stdout; std_err = sys.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wave_600 = np.arange(4000, 11000, .65) \n",
    "new_wave_1200 = np.arange(6000, 11000, .33) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_data(file_names,mask_name):\n",
    "    \n",
    "    tot_flux = []\n",
    "    tot_wave = []\n",
    "    tot_ivar = []\n",
    "    \n",
    "    for j in range(len(file_names)):\n",
    "        #read in star data\n",
    "        h_star = fits.open(path_name + '/' + 'data/{0}'.format(mask_name) + '/' + file_names[j], ignore_missing_end = True)\n",
    "        data_star1 = h_star[1].data\n",
    "        star_flux1 = data_star1['SKYSPEC'][0]\n",
    "        star_wave1 = data_star1['LAMBDA'][0]\n",
    "        star_ivar1 = data_star1['IVAR'][0]\n",
    "        \n",
    "        data_star2 = h_star[2].data\n",
    "        star_flux2 = data_star2['SKYSPEC'][0]\n",
    "        star_wave2 = data_star2['LAMBDA'][0]\n",
    "        star_ivar2 = data_star2['IVAR'][0]\n",
    "        \n",
    "        \n",
    "        #combine the blue and red side into one list\n",
    "        star_flux = np.array(list(star_flux1) + list(star_flux2))\n",
    "        star_wave = np.array(list(star_wave1) + list(star_wave2))\n",
    "        star_ivar = np.array(list(star_ivar1) + list(star_ivar2))\n",
    "        \n",
    "        #add to above lists\n",
    "        tot_flux.append(star_flux)\n",
    "        tot_wave.append(star_wave)\n",
    "        tot_ivar.append(star_ivar)\n",
    "\n",
    "        h_star.close()\n",
    "        \n",
    "    return tot_flux, tot_wave, tot_ivar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebin(fluxes, waves, ivar, grating):\n",
    "    \n",
    "    rbflux = []\n",
    "    rbivar = []\n",
    "    \n",
    "    if grating == 600:\n",
    "        new_wave = new_wave_600\n",
    "    elif grating == 1200:\n",
    "        new_wave = new_wave_1200\n",
    "    \n",
    "    for i in range(len(waves)):\n",
    "        new_flux,new_ivar = py_specrebin.rebinspec(waves[i],fluxes[i],new_wave,ivar=ivar[i])\n",
    "        new_flux_err = 1/np.sqrt(new_ivar)\n",
    "\n",
    "        rbflux.append(new_flux)\n",
    "        rbivar.append(new_ivar)\n",
    "        \n",
    "    return rbflux, new_wave, rbivar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_median(rebinned_flux_array):\n",
    "    \n",
    "    median_vals = []\n",
    "    \n",
    "    print(len(rebinned_flux_array))\n",
    "    \n",
    "    for i in range(len(rebinned_flux_array[0])):\n",
    "\n",
    "        comp = []\n",
    "        \n",
    "        for array in rebinned_flux_array:\n",
    "            \n",
    "            if np.isfinite(array[i]) == True:\n",
    "                comp.append(array[i])\n",
    "                \n",
    "        median_vals.append(np.median(comp))\n",
    "        \n",
    "    return median_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exclusions():\n",
    "    filepath = 'ISM_EM_LINES.txt'\n",
    "    fp = open(filepath)\n",
    "    all_data = []\n",
    "    for line in (fp):\n",
    "        mask_name = line.split(':')[0].split('_')[0]\n",
    "        slit_number = line.split(':')[1].strip().split(\" \")[0]\n",
    "        if len(slit_number) == 2:\n",
    "            slit_number = '0' + slit_number\n",
    "        elif len(slit_number) == 1:\n",
    "            slit_number = '00' + slit_number\n",
    "        else:\n",
    "            pass\n",
    "        object_id = line.split(':')[1].strip().split()[1]\n",
    "        data = {}\n",
    "        data['mask_name'] = mask_name\n",
    "        data['slit_number'] = slit_number\n",
    "        data['object_id'] = object_id\n",
    "        all_data.append(data)\n",
    "    return all_data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_to_include(folder):\n",
    "    import os\n",
    "    list_of_files_to_include = []\n",
    "    list_of_files_to_exclude = []\n",
    "    serendip_files = []\n",
    "    all_file_names_in_folder = os.listdir('data/{}'.format(folder))\n",
    "    y = len(all_file_names_in_folder)\n",
    "    print(\"The number of files in the folder is {0}\".format(y))\n",
    "    all_data = get_exclusions()\n",
    "    len_all_data = len(all_data)\n",
    "    for n in range(y):\n",
    "        parts_of_file_name = all_file_names_in_folder[n].split(\".\")\n",
    "        if parts_of_file_name[0] == 'spec1d': # avoids hidden DS_Store files on my mac\n",
    "            object_id = parts_of_file_name[3]\n",
    "            slit_number = parts_of_file_name[2]\n",
    "            mask_name = parts_of_file_name[1]\n",
    "            should_include = True\n",
    "            should_exclude = True\n",
    "            for k in range(len_all_data):\n",
    "                if ((object_id == all_data[k]['object_id']) and (slit_number == all_data[k]['slit_number']) and (mask_name == all_data[k]['mask_name'])):\n",
    "                    should_include = False\n",
    "                    should_exclude = True\n",
    "                if 'serendip' in object_id:\n",
    "                    should_include = False\n",
    "                    should_exclude = False\n",
    "            if should_include == True:\n",
    "                list_of_files_to_include.append(all_file_names_in_folder[n])       \n",
    "            elif should_exclude == True:\n",
    "                list_of_files_to_exclude.append(all_file_names_in_folder[n])\n",
    "            elif should_include == False & should_exclude == False:\n",
    "                serendip_files.append(all_file_names_in_folder[n])\n",
    "    \n",
    "    print('The number of files left after exclusions is {0}'.format(len(list_of_files_to_include)))\n",
    "    \n",
    "    return sorted(list_of_files_to_include), sorted(list_of_files_to_exclude), sorted(serendip_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Save The Rebinned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sarthak's function as modified by Liv Gaunt\n",
    "def exportToFits(rbflux, rbwave, rbivar, mask_name, file_names, incl_or_excl):\n",
    "\n",
    "    for i in range(len(rbflux)):\n",
    "            \n",
    "        hdu1 = fits.PrimaryHDU() #primary HDU (empty)\n",
    "        hdu1.header['INCLUDE'] = (incl_or_excl, 'Include in median calc if T') #this sets the tag for inclusion\n",
    "            \n",
    "        c1 = fits.Column(name='RBFLUX', array=rbflux[i], format='E')\n",
    "        c2 = fits.Column(name='RBWAVE', array=rbwave, format='E') #no [i] on rbwave since it's just one array\n",
    "        c3 = fits.Column(name='RBIVAR', array=rbivar[i], format='E')\n",
    "        hdu2 = fits.BinTableHDU.from_columns([c1, c2, c3]) #first extensional HDU (w data)\n",
    "            \n",
    "        hdul = fits.HDUList([hdu1, hdu2]) #combine both HDUs into file and write it below\n",
    "            \n",
    "        #this part puts the files to include in one folder, and those to exclude in another\n",
    "        if incl_or_excl == True:\n",
    "            hdul.writeto(path_name + '/{0}_Rebinned/{0}_Included'.format(mask_name) + '/' + 'rebinned_{0}'.format(file_names[i]))\n",
    "            \n",
    "        elif incl_or_excl == False:\n",
    "            hdul.writeto(path_name + '/{0}_Rebinned/{0}_Excluded'.format(mask_name) + '/' + 'rebinned_{0}'.format(file_names[i]))\n",
    "                \n",
    "        else:\n",
    "            hdul.writeto(path_name + '/{0}_Rebinned/{0}_Serendip'.format(mask_name) + '/' + 'rebinned_{0}'.format(file_names[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Read FITS File and Get Back Rebin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fits_rebinned_data (mask_name, file_names, incl_or_excl):\n",
    "    if incl_or_excl == True:\n",
    "        \n",
    "        #all of these are libraries\n",
    "        \n",
    "        rbflux = []\n",
    "        rbwave = []\n",
    "        rbivar = []\n",
    "        \n",
    "        for slit in file_names: \n",
    "            rb_fits_include = fits.open(path_name + \"/{0}_Rebinned/{0}_Included/rebinned_{1}\".format(mask_name,slit))\n",
    "            rbflux.append(rb_fits_include[1].data[\"RBFLUX\"])\n",
    "            rbwave.append(rb_fits_include[1].data[\"RBWAVE\"])\n",
    "            rbivar.append(rb_fits_include[1].data[\"RBIVAR\"])\n",
    "            \n",
    "    elif incl_or_excl == False: \n",
    "        \n",
    "        rbflux = []\n",
    "        rbwave = []\n",
    "        rbivar = []\n",
    "        \n",
    "        for slit in file_names: \n",
    "            rb_fits_include = fits.open(path_name + \"/{0}_Rebinned/{0}_Excluded/rebinned_{1}\".format(mask_name,slit))\n",
    "            rbflux.append(rb_fits_include[1].data[\"RBFLUX\"])\n",
    "            rbwave.append(rb_fits_include[1].data[\"RBWAVE\"])\n",
    "            rbivar.append(rb_fits_include[1].data[\"RBIVAR\"]) \n",
    "        \n",
    "    else: \n",
    "        \n",
    "        rbflux = []\n",
    "        rbwave = []\n",
    "        rbivar = []\n",
    "        \n",
    "        for slit in file_names: \n",
    "            rb_fits_include = fits.open(path_name + \"/{0}_Rebinned/{0}_Serendip/rebinned_{1}\".format(mask_name,slit))\n",
    "            rbflux.append(rb_fits_include[1].data[\"RBFLUX\"])\n",
    "            rbwave.append(rb_fits_include[1].data[\"RBWAVE\"])\n",
    "            rbivar.append(rb_fits_include[1].data[\"RBIVAR\"]) \n",
    "        \n",
    "    return rbflux, rbwave, rbivar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Save The Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportToFitsMedian(median,mask_name):\n",
    "    \n",
    "    hdu1 = fits.PrimaryHDU()\n",
    "        \n",
    "    c1 = fits.Column(name='MEDIAN',array=median,format=\"E\")\n",
    "    hdu2 = fits.BinTableHDU.from_columns([c1])\n",
    "        \n",
    "    hdul = fits.HDUList([hdu1,hdu2])\n",
    "        \n",
    "    hdul.writeto(path_name + '/{0}_Median/Median_of_{0}.fits.gz'.format(mask_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function To Read and Get Back Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_med_from_fits(mask_name):\n",
    "    median_read = fits.open(path_name + '/{0}_Median/Median_of_{0}.fits.gz'.format(mask_name))\n",
    "    median_fits = median_read[1].data[\"MEDIAN\"] #contain the median \n",
    "    return median_fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median Airglow Subtraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_subtraction(slit_index,rebinned_flux):\n",
    "    \n",
    "    new_flux = []\n",
    "    \n",
    "    spectrum = rebinned_flux[slit_index]\n",
    "   \n",
    "    for i in range(len(spectrum)):\n",
    "        if np.isfinite(spectrum[i]) == True:\n",
    "            new_flux.append(spectrum[i] - median[i])\n",
    "        else:\n",
    "            new_flux.append(spectrum[i])\n",
    "            \n",
    "    return new_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slit_nums(files):\n",
    "    \n",
    "    slit_nums = []\n",
    "    \n",
    "    if len(files) > 1:\n",
    "    \n",
    "        for i in range(len(files)):\n",
    "            parts_of_file_name = files[i].split(\".\")\n",
    "            slit_num = parts_of_file_name[2]\n",
    "            slit_nums.append(int(slit_num))\n",
    "            \n",
    "    return slit_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_slit_index(slit_nums,slit_num): \n",
    "    #print('The index of slit number {} is: '.format(slit_num), slit_nums.index(slit_num))\n",
    "    return slit_nums.index(slit_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(mask_name, slit_nums, rebinned_flux, median, incl_or_excl):\n",
    "    \n",
    "    if incl_or_excl == True:\n",
    "        for slit in slit_nums:\n",
    "            slit_index = find_slit_index(slit_nums,slit)\n",
    "            new_flux = median_subtraction(slit_index,rebinned_flux)\n",
    "            fig,axs = plt.subplots(1)\n",
    "            fig.patch.set_alpha(1)\n",
    "            plt.ylim(-10000,10000) #could try getting a smaller y limit and getting rid of legend\n",
    "            plt.xlim(4000,11000)\n",
    "            #plt.plot(wave_all[slit_index], flux_all[slit_index], color = 'r', label = 'original')\n",
    "            plt.plot(new_wave_600, median, scalex=False, scaley=False, color = 'r', label = 'median')\n",
    "            plt.plot(new_wave_600, rebinned_flux[slit_index] + 4000, scalex=False, scaley=False, color = 'b', label = 'rebinned')\n",
    "            plt.plot(new_wave_600, np.array(new_flux) + 8000, scalex=False, scaley=False, color = 'g', label = 'median subtracted')\n",
    "            plt.title('{} Slit {}'.format(mask_name, slit))\n",
    "            plt.xlabel('Wavelength')\n",
    "            plt.ylabel('Flux')\n",
    "            plt.legend()\n",
    "            fig.savefig('{0}_Spectra/{0}_Included/{0} Slit {1}'.format(mask_name, slit)) #folder name would need to change for each mask\n",
    "    else:\n",
    "        for slit in slit_nums:\n",
    "            slit_index = find_slit_index(slit_nums,slit)\n",
    "            new_flux = median_subtraction(slit_index,rebinned_flux)\n",
    "            fig,axs = plt.subplots(1)\n",
    "            fig.patch.set_alpha(1)\n",
    "            plt.ylim(-10000,10000) #could try getting a smaller y limit and getting rid of legend\n",
    "            plt.xlim(4000,11000)\n",
    "            #plt.plot(wave_all[slit_index], flux_all[slit_index], color = 'r', label = 'original')\n",
    "            plt.plot(new_wave_600, median, scalex=False, scaley=False, color = 'r', label = 'median')\n",
    "            plt.plot(new_wave_600, rebinned_flux[slit_index] + 4000, scalex=False, scaley=False, color = 'b', label = 'rebinned')\n",
    "            plt.plot(new_wave_600, np.array(new_flux) + 8000, scalex=False, scaley=False, color = 'g', label = 'median subtracted')\n",
    "            plt.title('{} Slit {}'.format(mask_name, slit))\n",
    "            plt.xlabel('Wavelength')\n",
    "            plt.ylabel('Flux')\n",
    "            plt.legend()\n",
    "            fig.savefig('{0}_Spectra/{0}_Excluded/{0} Slit {1}'.format(mask_name, slit)) #folder name would need to change for each mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking At Specific Slit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_slit(slit_number,rebinned_flux,median,incl_or_excl,multiplier,\n",
    "             min_ylim,max_ylim,min_xlim,max_xlim): #combination of median_subtraction + plotting function \n",
    "#slit_number, rebinned_flux, and incl_excl will need to be changed depending on whether we want to look at incl or excl \n",
    "    if incl_or_excl == True: #median subtraction \n",
    "        new_flux = [] #sky subtracted spectra \n",
    "        slit_index = find_slit_index(slit_nums,slit_number)\n",
    "        spectrum = rebinned_flux[slit_index]\n",
    "        multiplier = multiplier \n",
    "\n",
    "        for i in range(len(spectrum)):\n",
    "            if np.isfinite(spectrum[i]) == True:\n",
    "                new_flux.append((spectrum[i]*multiplier) - median[i]) \n",
    "            else:\n",
    "                new_flux.append(spectrum[i])\n",
    "\n",
    "        #plotting \n",
    "        fig,axs = plt.subplots(1)\n",
    "        plt.ylim(-10000,10000) #could try getting a smaller y limit and getting rid of legend\n",
    "        plt.xlim(4000,11000)\n",
    "        plt.plot(new_wave_600, median, c=\"r\", scalex=False, scaley=False, label = \"median\")\n",
    "        plt.plot(new_wave_600, rebinned_flux[slit_index] + 5000, c=\"b\", scalex=False, scaley=False, label = \"rebinned\")\n",
    "        #plt.plot(new_wave_600, rebinned_flux[\"slit_{}\".format(slit_number)] - 0.9*median + 8000, scalex=False, scaley=False,\n",
    "                #label = \"subtracted\") #error\n",
    "        plt.plot(new_wave_600, np.array(new_flux), c=\"g\", scalex=False, scaley=False,label = \"subtracted\")\n",
    "        plt.xlabel(\"Wavelength (A)\")\n",
    "        plt.ylabel(\"Flux (Electron/Hour)\")\n",
    "        plt.title(\"Slit #{}\".format(slit_number))\n",
    "        \n",
    "    else:\n",
    "        new_flux = [] #sky subtracted spectra w/ scaling\n",
    "        new_flux_no_scaling = [] #sky subtracted spectra w/o scaling\n",
    "        rbflux_with_scaling = []\n",
    "        slit_index = find_slit_index(slit_nums_exclude,slit_number) #changed slit_nums to slit_nums_exclude\n",
    "        spectrum = rebinned_flux[slit_index] #rbflux w/ no scaling\n",
    "        multiplier = multiplier \n",
    "\n",
    "        #multiplying the rbflux by scaling factor \n",
    "        for i in range(len(spectrum)):\n",
    "            if np.isfinite(spectrum[i]) == True:\n",
    "                new_flux.append((spectrum[i] * multiplier) - median[i]) \n",
    "                rbflux_with_scaling.append((spectrum[i] * multiplier))\n",
    "            else:\n",
    "                new_flux.append(spectrum[i])\n",
    "                rbflux_with_scaling.append(spectrum[i])\n",
    "\n",
    "        #no scaling factor\n",
    "        for i in range(len(spectrum)):\n",
    "            if np.isfinite(spectrum[i]) == True:\n",
    "                new_flux_no_scaling.append((spectrum[i]) - median[i]) \n",
    "            else:\n",
    "                new_flux_no_scaling.append(spectrum[i])\n",
    "        \n",
    "        #plotting \n",
    "        fig,axs = plt.subplots(1)\n",
    "        plt.ylim(min_ylim,max_ylim) #could try getting a smaller y limit and getting rid of legend\n",
    "        plt.xlim(min_xlim,max_xlim)\n",
    "        plt.plot(new_wave_600, median, c=\"r\", scalex=False, scaley=False, label = \"median\")\n",
    "        plt.plot(new_wave_600, rbflux_with_scaling, c=\"b\", scalex=False, scaley=False, label = \"rebinned w/ scaling\")\n",
    "        plt.plot(new_wave_600, np.array(new_flux_no_scaling), c=\"purple\", scalex=False, scaley=False, label = \"Subtracted w/o scaling\")\n",
    "        plt.plot(new_wave_600, np.array(new_flux), c=\"g\", scalex=False, scaley=False,label = \"subtracted\")\n",
    "        plt.xlabel(\"Wavelength (A)\")\n",
    "        plt.ylabel(\"Flux (Electron/Hour)\")\n",
    "        plt.title(\"Slit #{}\".format(slit_number) + \"/Multiplier: {}\".format(multiplier) \n",
    "                  + \"/From {0} to {1}\".format(min_xlim,max_xlim))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define The Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_name = \"C1M33P\" #change to fit the appropriate mask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Files We Want to Include and Exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering files\n",
    "list_of_files_to_include, list_of_files_to_exclude, list_of_serendip_files = get_files_to_include(mask_name)\n",
    "\n",
    "#sorted\n",
    "#file_names = all slits used to create the median (airglow)\n",
    "#file_names_exclude = all slits that contain ISM emission lines \n",
    "#file_names_serendip = all serendip files\n",
    "#file_names_all = all slits excluding \"serendip\"\n",
    "\n",
    "file_names = list_of_files_to_include\n",
    "file_names_exclude = list_of_files_to_exclude\n",
    "file_names_serendip = list_of_serendip_files\n",
    "file_names_all = list_of_files_to_include + list_of_files_to_exclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting The Wavelength, Flux, and Inverse Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to comment out the codes in this section after you have rebinned and saved your data!!!\n",
    "\n",
    "Then make sure to uncomment them whenever you're working with a new mask and want to rebin!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting data\n",
    "#try getting and rebinning all files\n",
    "flux, wave, ivar= get_original_data(file_names, mask_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebinning the original data\n",
    "rbflux, rbwave, rbivar = rebin(flux, wave, ivar, 600) # this takes about 4 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all excluded data\n",
    "flux_exclude, wave_exclude, ivar_exclude = get_original_data(file_names_exclude, mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebinning the excluded data\n",
    "rbflux_exclude, rbwave_exclude, rbivar_exclude = rebin(flux_exclude, wave_exclude, ivar_exclude, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all serendip data \n",
    "#NOTE: we will never use it but is good to just process it\n",
    "flux_serendip, wave_serendip, ivar_serendip = get_original_data(list_of_serendip_files, mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebinning the serendip data\n",
    "rbflux_serendip, rbwave_serendip, rbivar_serendip = rebin(flux_serendip, wave_serendip, ivar_serendip, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving The Rebinned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [#make three folders to store the rebinned data, the median, and the spectra\n",
    "        \"./{}_Rebinned\".format(mask_name),\n",
    "        \"./{}_Spectra\".format(mask_name),\n",
    "        \"./{}_Median\".format(mask_name),\n",
    "\n",
    "        #make sub-folders for rebinned data\n",
    "        \"./{0}_Rebinned/{0}_Excluded\".format(mask_name),\n",
    "        \"./{0}_Rebinned/{0}_Included\".format(mask_name),\n",
    "        \"./{0}_Rebinned/{0}_Serendip\".format(mask_name),\n",
    "\n",
    "        #make sub-folders for the spectra\n",
    "        \"./{0}_Spectra/{0}_Excluded\".format(mask_name),\n",
    "        \"./{0}_Spectra/{0}_Included\".format(mask_name),\n",
    "\n",
    "        #make directory to stores the scaled flux and shifted wavelength and polynomial coefficients\n",
    "        \"./{0}_Rebinned/{0}_Scale_Values\".format(mask_name),\n",
    "        \"./{0}_Rebinned/{0}_Shift_Values\".format(mask_name),\n",
    "        \"./{0}_Rebinned/{0}_Polynomial_Coefficients\".format(mask_name),\n",
    "\n",
    "        #make sub-folders for scaling and shiftingfFactors and rebinned flux w/ shifted wavelength\n",
    "        \"./{0}_Rebinned/{0}_Scaling_and_Shifting_Factor\".format(mask_name),\n",
    "        \"./{0}_Rebinned/{0}_Rebinned_Flux_Shifted_Wave\".format(mask_name),\n",
    "\n",
    "        #make directories to stores final trimmed spectra\n",
    "        \"./{0}_Trimmed_Spectra\".format(mask_name),\n",
    "        \"./{0}_Trimmed_Spectra/Excluded\".format(mask_name),\n",
    "        \"./{0}_Trimmed_Spectra/Included\".format(mask_name),\n",
    "        \"./{0}_Trimmed_Spectra/Optimized_Spectrum_Flux\".format(mask_name),\n",
    "\n",
    "        #make directories to stores polynomial fits and factor vs RMS plots\n",
    "        \"./{0}_Polynomial_Graph\".format(mask_name),\n",
    "        \"./{0}_Polynomial_Graph/Scaling_vs_RMS\".format(mask_name),\n",
    "        \"./{0}_Polynomial_Graph/Shifting_vs_RMS\".format(mask_name),\n",
    "        \"./{0}_Polynomial_Graph/Scaling_Fitting\".format(mask_name),\n",
    "        \"./{0}_Polynomial_Graph/Shifting_Fitting\".format(mask_name)]\n",
    "\n",
    "for path in paths:\n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exportToFits(rbflux, rbwave, rbivar, mask_name, file_names, True) \n",
    "exportToFits(rbflux_exclude, rbwave_exclude, rbivar_exclude, mask_name, file_names_exclude, False)\n",
    "exportToFits(rbflux_serendip, rbwave_serendip, rbivar_serendip, mask_name, file_names_serendip, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Back The Rebinned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbflux_fits,rbwave_fits,rbivar_fits = get_fits_rebinned_data(mask_name,file_names,True)\n",
    "rbflux_fits_exclude,rbwave_fits_exclude,rbivar_fits_exclude = get_fits_rebinned_data(mask_name,file_names_exclude,False)\n",
    "rbflux_fits_serendip,rbwave_fits_serendip,rbivar_fits_serendip = get_fits_rebinned_data(mask_name,file_names_serendip,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding The Median "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking the median\n",
    "median = find_median(rbflux_fits) #median length is 10770 (M33D2A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Median As FITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportToFitsMedian(median,mask_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Back Median From FITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_fits = get_med_from_fits(mask_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slits to Include and Exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slit_nums = get_slit_nums(file_names)\n",
    "slit_nums_exclude = get_slit_nums(file_names_exclude)\n",
    "\n",
    "all_slit_nums = get_slit_nums(file_names_all)\n",
    "\n",
    "print(\"Slit # to INCLUDE in median calculation: {0}\".format(slit_nums))\n",
    "print(\"Slit # to EXCLUDE: {0}\".format(slit_nums_exclude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_array_func(obs_wave,rest_wave):\n",
    "    return ((obs_wave/rest_wave) - 1) * 299792"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_wavelength = new_wave_600[np.where((new_wave_600 > 6500) & (new_wave_600 < 6800))]\n",
    "master_velocity_array = np.arange(-700,110,10)\n",
    "H_alpha_v_arr = v_array_func(obs_wavelength,6562.82)\n",
    "N_II_1_v_arr = v_array_func(obs_wavelength,6548.10)\n",
    "N_II_2_v_arr = v_array_func(obs_wavelength,6583.60)\n",
    "S_II_1_v_arr = v_array_func(obs_wavelength,6716.47)\n",
    "S_II_2_v_arr = v_array_func(obs_wavelength,6730.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbflux_list = []\n",
    "for rbflux in rbflux_exclude:\n",
    "    rbflux_list.append(rbflux[np.where((new_wave_600 > 6500) & (new_wave_600 < 6800))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_list = []\n",
    "for n in range(len(new_wave_600[np.where((new_wave_600 > 6500) & (new_wave_600 < 6800))])):\n",
    "    values_for_std_cal = []\n",
    "    for rbflux in rbflux_list:\n",
    "        values_for_std_cal.append(rbflux[n])\n",
    "    std_list.append(statistics.stdev(values_for_std_cal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_ivar = 1/(np.array(std_list)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebin_vel(fluxes, velocity, ivar, grating):\n",
    "    \n",
    "    rbflux = []\n",
    "    rbivar = []\n",
    "    \n",
    "    if grating == 600:\n",
    "        new_velocity = master_velocity_array\n",
    "    elif grating == 1200:\n",
    "        new_velocity = master_velocity_array\n",
    "    \n",
    "    for i in range(len(velocity)):\n",
    "        new_flux,new_ivar = py_specrebin.rebinspec(velocity[i],fluxes[i],new_velocity,ivar=ivar[i])\n",
    "        new_flux_err = 1/np.sqrt(new_ivar)\n",
    "\n",
    "        rbflux.append(new_flux)\n",
    "        rbivar.append(new_ivar)\n",
    "        \n",
    "    return rbflux, new_velocity, rbivar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_list = [rbflux_list[5],rbflux_list[5],rbflux_list[5],rbflux_list[5],rbflux_list[5]]\n",
    "vel_list = [H_alpha_v_arr,N_II_1_v_arr,N_II_2_v_arr,S_II_1_v_arr,S_II_2_v_arr]\n",
    "ivar_list = [v_ivar,v_ivar,v_ivar,v_ivar,v_ivar]\n",
    "rbflux_vel,rbvel,rbivar_v = rebin_vel(flux_list,vel_list,ivar_list,600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sum = ((sum(rbivar_v[0])*rbflux_vel[0]) + (sum(rbivar_v[1])*rbflux_vel[1]) + (sum(rbivar_v[2])*rbflux_vel[2]) + (sum(rbivar_v[3])*rbflux_vel[3]) + (sum(rbivar_v[4])*rbflux_vel[4]))/(sum(rbivar_v[0])+sum(rbivar_v[1])+sum(rbivar_v[2])+sum(rbivar_v[3])+sum(rbivar_v[4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(master_velocity_array,weighted_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting ALL Slits In A Mask and Saving It In A Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot all slits and save it as a png in a folder \n",
    "#Need to change the mask name for each mask \n",
    "plotting(mask_name, slit_nums, rbflux_fits, median_fits, True)\n",
    "plotting(mask_name, slit_nums_exclude, rbflux_fits_exclude, median_fits, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Starlight Contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_median_new(a, window=50, reverse=False, center_align=False):\n",
    "    \n",
    "    '''\n",
    "    Returns the moving median values up to the halfway point of the array\n",
    "    \n",
    "    Version - 3.0\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : ndarray\n",
    "        One dimensional flux array.\n",
    "    window : int, optional\n",
    "        The size of each segment for taking the median.\n",
    "    reverse : bool, optional\n",
    "        Can be used to switch between left-to-right\n",
    "        and right-to-left directions for the moving median.\n",
    "        Default is left-to-right.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    median_arr : One dimensional array of moving RMS.\n",
    "    '''\n",
    "    \n",
    "    median_arr = []\n",
    "    b = np.where(np.isfinite(a))[0]\n",
    "    mid_index = int((b[-1]+b[0])/2)\n",
    "    \n",
    "    if (center_align==True):\n",
    "        p,q,r = max(b[0],window//2+1), min(b[-1]+1,len(a)-window//2), 1\n",
    "        j,k,l = -window//2, window//2+1, 1\n",
    "    else:\n",
    "        if reverse==False:\n",
    "            p,q,r = 0, mid_index, 1\n",
    "            j,k,l = 0, window, 1\n",
    "        else:\n",
    "            p,q,r = mid_index+2, len(a), 1\n",
    "            j,k,l = 0, -window, -1\n",
    "    \n",
    "    for i in range(p,q,r):\n",
    "        x = a[i+j:i+k:l]\n",
    "        median = np.nanmedian(x[np.where(np.isfinite(x))[0]])\n",
    "        median_arr.append(median)\n",
    "    \n",
    "    if (center_align==True):\n",
    "        median_arr = [np.nan]*p + median_arr + [np.nan]*(len(a)-q)\n",
    "    \n",
    "    return np.array(median_arr)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_median_of_median = moving_median_new(np.array(median),window=325,center_align=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylim(-200,200)\n",
    "plt.plot(new_wave_600,median)\n",
    "plt.plot(new_wave_600,moving_median_of_median)\n",
    "plt.plot(new_wave_600,median-moving_median_of_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_median_of_sky_sub_spec = moving_median_new(rbflux_exclude[3]-np.array(median),window=325,center_align=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(new_wave_600,rbflux_exclude[3]-np.array(median))\n",
    "plt.plot(new_wave_600,moving_median_of_sky_sub_spec)\n",
    "plt.plot(new_wave_600,(rbflux_exclude[3]-np.array(median))-moving_median_of_sky_sub_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_median = 150 #define the threshold for median boolean\n",
    "threshold_sky_sub = 50 #define the threshold for sky subtraction boolean \n",
    "\n",
    "multipliers = np.arange(0.1,2.01,0.01) #array of multipliers we want to test\n",
    "shift_test_values = np.arange(-0.2, 0.21, 0.01) #range of shift values we want to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slits That Are INCLUDED in The Median Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPROVEMENT: The indexing issues. Whenever we are analyzing a new part of the graph, we have to keep changing and finding the index. Find a way to make this more efficient!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_threshold(median, threshold):\n",
    "    \n",
    "    moving_median_of_median = moving_median_new(np.array(median),window=325,center_align=True)\n",
    "    \n",
    "    no_starlight_median = np.array(median) - np.array(moving_median_of_median)\n",
    "    \n",
    "    median_boolean = []\n",
    "    \n",
    "    for value in no_starlight_median:\n",
    "        \n",
    "        if np.isfinite(value) == True: #to filter out nan \n",
    "            \n",
    "            if value > threshold:\n",
    "                median_boolean.append(True)\n",
    "                \n",
    "            elif value <= threshold:\n",
    "                median_boolean.append(False)\n",
    "            \n",
    "        else:\n",
    "            median_boolean.append(False)\n",
    "            \n",
    "    median_boolean_array = np.array(median_boolean)\n",
    "    \n",
    "    return median_boolean_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wave_bool(wavelength, min_wave, max_wave):\n",
    "    \n",
    "    wavelength_boolean = []\n",
    "    \n",
    "    for value in wavelength: \n",
    "        \n",
    "        if (value > min_wave) and (value < max_wave): \n",
    "            \n",
    "            wavelength_boolean.append(True)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            wavelength_boolean.append(False)\n",
    "            \n",
    "    wavelength_boolean_array = np.array(wavelength_boolean)\n",
    "            \n",
    "    return wavelength_boolean_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sky_sub_bool(slit_index, rebinned_flux_list, median, threshold):\n",
    "    new_flux = [] #sky subtracted spectra \n",
    "    #slit_index = find_slit_index(slit_nums_exclude,slit_number) #changed slit_nums to slit_nums_exclude\n",
    "    slit_index = slit_index\n",
    "    spectrum = rebinned_flux_list[slit_index]\n",
    "\n",
    "    for i in range(len(spectrum)):\n",
    "        if np.isfinite(spectrum[i]) == True:\n",
    "            new_flux.append(spectrum[i] - median[i]) \n",
    "        else:\n",
    "            new_flux.append(spectrum[i])\n",
    "    \n",
    "    #rbflux - median is stored as new_flux \n",
    "   \n",
    "\n",
    "    moving_median_of_sky_sub_spec = moving_median_new(np.array(new_flux),window=325,center_align=True)\n",
    "    \n",
    "    no_starlight_skysub_rbflux = np.array(new_flux) - np.array(moving_median_of_sky_sub_spec)\n",
    "    \n",
    "    rbflux_boolean =[]\n",
    "    \n",
    "    for value in no_starlight_skysub_rbflux: #loop through all values in new_flux and determine it's above the threshold\n",
    "    \n",
    "        if np.isfinite(value) == True: #to filter out nan \n",
    "\n",
    "            if value > threshold:\n",
    "                    rbflux_boolean.append(False) #if above 150 e/hr, exclude that pixel\n",
    "\n",
    "            elif value <= threshold:\n",
    "                    rbflux_boolean.append(True) #if below 150 e/hr, include that pixel\n",
    "        \n",
    "        else:\n",
    "            rbflux_boolean.append(False) #if is a nan value, exclude it \n",
    "            \n",
    "    return np.array(rbflux_boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbflux_minus_median(rbflux, median, multipliers):\n",
    "    \n",
    "    subtraction_dict = {}\n",
    "    \n",
    "    for multiplier in multipliers: \n",
    "    \n",
    "        subtraction_list = (multiplier * np.array(rbflux)) - median\n",
    "    \n",
    "        #for n in range(len(rbflux)): \n",
    "\n",
    "         #   if np.isfinite(rbflux[n]) == True: #rbflux contains nan \n",
    "\n",
    "          #      subtraction = (multiplier * rbflux[n]) - median[n]\n",
    "\n",
    "           #     subtraction_list.append(subtraction)\n",
    "\n",
    "            #else:\n",
    "             #   subtraction_list.append(multiplier * rbflux[n])\n",
    "    \n",
    "        subtraction_dict[\"Multiplier_{}\".format(round(multiplier,2))] = subtraction_list\n",
    "    \n",
    "    return subtraction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_rms(multiply_boolean, subtraction_dict, multipliers): #both inputs are same length\n",
    "    \n",
    "    rms_dict_sorted = {}\n",
    "    \n",
    "    for multiplier in multipliers:\n",
    "        \n",
    "        #rms_values = []\n",
    "        \n",
    "        subtraction = subtraction_dict[\"Multiplier_{}\".format(round(multiplier,2))]\n",
    "    \n",
    "        #for n in range(len(multiply_boolean)):\n",
    "\n",
    "         #   if multiply_boolean[n] == True: \n",
    "\n",
    "          #      rms_values.append(subtraction[n])\n",
    "\n",
    "           # else:\n",
    "\n",
    "            #    pass\n",
    "            \n",
    "        rms_dict_sorted[\"Multiplier_{}\".format(round(multiplier,2))] = np.array(subtraction)[np.where(multiply_boolean == True)]\n",
    "                    \n",
    "    return rms_dict_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_calculation(rms_dict_sorted, multipliers):\n",
    "    \n",
    "    try:\n",
    "        rms_dict = {}\n",
    "\n",
    "        for multiplier in multipliers:\n",
    "\n",
    "            values_for_rms_cal = rms_dict_sorted[\"Multiplier_{}\".format(round(multiplier,2))]\n",
    "            \n",
    "            rms = statistics.stdev(values_for_rms_cal)\n",
    "\n",
    "            rms_dict[\"Multiplier_{}\".format(round(multiplier,2))] = rms\n",
    "        \n",
    "        return rms_dict\n",
    "    \n",
    "    except:\n",
    "        print(\"Everything is False. There's no True boolean. Therefore, RMS cannot be calculated.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_the_rms(slit_number,multipliers, rms_dict, min_wave, max_wave):\n",
    "\n",
    "    path = \"./{0}_Polynomial_Graph/Scaling_vs_RMS/Slit_{1}\".format(mask_name,slit_number)\n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise\n",
    "    try:\n",
    "        value_list = []\n",
    "\n",
    "        for multiplier in multipliers: \n",
    "\n",
    "            value_list.append(rms_dict[\"Multiplier_{0}\".format(round(multiplier,2))])\n",
    "\n",
    "        fig = plt.figure(figsize=(8,6))\n",
    "        fig.patch.set_alpha(1)\n",
    "        plt.plot(multipliers,value_list)\n",
    "        plt.xlabel(\"Scaling\")\n",
    "        plt.ylabel(\"RMS\")\n",
    "        plt.title(\"Mask {0}: Slit #{1}\\nRMS vs Scaling ({2} A to {3} A)\".format(mask_name,slit_number,min_wave,max_wave))\n",
    "        fig.savefig(path+'/{0}_Slit_{1}_{2}_to_{3}.png'.format(mask_name,slit_number,min_wave,max_wave))\n",
    "        \n",
    "        min_val = min(value_list)\n",
    "\n",
    "        scale = round(multipliers[value_list.index(min_val)],2)\n",
    "\n",
    "        print(\"Scaling w/ minimum RMS (Slit #{0}): {1}\".format(slit_number,scale) + \" ({0} A to {1} A)\".format(min_wave,max_wave))\n",
    "        \n",
    "        return scale\n",
    "        \n",
    "    except:\n",
    "        print(\"Because we have no RMS there is no plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating All The Functions Used In Scaling Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_scaling(slit_number,median,threshold_median,threshold_sky_sub,\n",
    "                    wavelength,min_wave,max_wave,rbflux,multipliers,slit_index,subtraction_dict): \n",
    "    \n",
    "    #all the functions combined together here for convenience!\n",
    "    \n",
    "    #boolean array using median and threshold\n",
    "    median_boolean_array = median_threshold(median, threshold_median)\n",
    "    \n",
    "    #boolean array using wavelength\n",
    "    wavelength_boolean_array = create_wave_bool(wavelength, min_wave, max_wave)\n",
    "    \n",
    "    #boolean array using rbflux - median \n",
    "    sky_sub_boolean_array = sky_sub_bool(slit_index,rbflux,median,threshold_sky_sub)\n",
    "    \n",
    "    #multiply the two boolean arrays\n",
    "    multiply_boolean = median_boolean_array * wavelength_boolean_array * sky_sub_boolean_array\n",
    "    \n",
    "    #calculate all the subtraction using different scaling factor\n",
    "    #subtraction_dict = rbflux_minus_median(multiply_boolean, rbflux_fits_exclude[index_of_slit], \n",
    "                                           #median, multipliers)\n",
    "    subtraction_dict = subtraction_dict\n",
    "    \n",
    "    #sort through all the subtractions and keep only those that are True\n",
    "    rms_dict_sorted = sorting_rms(multiply_boolean, subtraction_dict, multipliers)\n",
    "    \n",
    "    #calculate the RMS associated with each scaling factor \n",
    "    rms_dict = rms_calculation(rms_dict_sorted, multipliers)\n",
    "    \n",
    "    #plot RMS vs scaling factor\n",
    "    scaling_value = plotting_the_rms(slit_number,multipliers,rms_dict,min_wave,max_wave)\n",
    "\n",
    "    return scaling_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looping_finding_scale_included_slits(slit_number,index_of_slit,subtraction_dict):\n",
    "\n",
    "    #wavelength_array =  np.arange(4000,11200,200)\n",
    "    wavelength_array = np.arange(4000,11500,500)\n",
    "    #wavelength_array = np.arange(4000, 11350, 350)\n",
    "    \n",
    "    scaling_value_dict = {}\n",
    "\n",
    "    for index in range(len(wavelength_array)):\n",
    "\n",
    "        if (index + 1) == len(wavelength_array): \n",
    "            break\n",
    "\n",
    "        else:\n",
    "        \n",
    "            scaling_value = finding_scaling(slit_number,median,threshold_median,threshold_sky_sub,rbwave_fits[0],wavelength_array[index],\n",
    "                                            wavelength_array[index+1],rbflux_fits,multipliers,index_of_slit,subtraction_dict)\n",
    "            \n",
    "            scaling_value_dict[\"{0}_to_{1}\".format(wavelength_array[index],wavelength_array[index+1])] = scaling_value\n",
    "            \n",
    "    return scaling_value_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Fits and Weighted Wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_first_order(x,poly_const):\n",
    "    return (x * poly_const[0]) + (poly_const[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_second_order(x,poly_const): #function that represent the third-order polynomial \n",
    "    return (x**2 * poly_const[0]) + (x * poly_const[1]) + (poly_const[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_third_order(x,poly_const): #function that represent the third-order polynomial \n",
    "    return (x**3 * poly_const[0]) + (x**2 * poly_const[1]) + (x * poly_const[2]) + (poly_const[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_fourth_order(x,poly_const): #function that represent the fourth-order polynomial \n",
    "    return (x**4 * poly_const[0]) + (x**3 * poly_const[1]) + (x**2 * poly_const[2]) + (x * poly_const[3]) + (poly_const[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_wave(median,threshold_median,threshold_sky_sub,wavelength,min_wave,max_wave,rbflux,slit_index):\n",
    "    \n",
    "    #boolean array using median and threshold\n",
    "    median_boolean_array = median_threshold(median, threshold_median)\n",
    "    \n",
    "    #boolean array using wavelength\n",
    "    wavelength_boolean_array = create_wave_bool(wavelength, min_wave, max_wave)\n",
    "    \n",
    "    #boolean array using rbflux - median \n",
    "    sky_sub_boolean_array = sky_sub_bool(slit_index,rbflux,median,threshold_sky_sub)\n",
    "    \n",
    "    #multiply the two boolean arrays\n",
    "    multiply_boolean = median_boolean_array * wavelength_boolean_array * sky_sub_boolean_array\n",
    "    \n",
    "    median_values_for_weight = [] #determine the median we want to use as our weight\n",
    "    median_values_index = []\n",
    "    \n",
    "    for n in range(len(multiply_boolean)):\n",
    "        \n",
    "        if multiply_boolean[n] == True:\n",
    "            \n",
    "            median_values_for_weight.append(median[n])\n",
    "            median_values_index.append(n)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            pass\n",
    "    \n",
    "    wavelength_values_for_weight = [] #determine the wavelength we want to use in \n",
    "    \n",
    "    for index in median_values_index:\n",
    "        \n",
    "        wavelength_values_for_weight.append(wavelength[index])\n",
    "    \n",
    "    #calculating the weighted wavelength\n",
    "    if len(median_values_for_weight) == 0:\n",
    "        print(\"All boolean values are False. Weighted wavelength cannot be calculate! (Wavelength {0} to {1})\".format(min_wave,max_wave))\n",
    "    \n",
    "    elif len(median_values_for_weight) == len(wavelength_values_for_weight): #make sure they have same length\n",
    "        \n",
    "        sigma_med_wave = []\n",
    "        \n",
    "        for n in range(len(median_values_for_weight)):\n",
    "            \n",
    "            sigma_med_wave.append(median_values_for_weight[n] * wavelength_values_for_weight[n])\n",
    "        \n",
    "        weighted_wavelength = sum(np.array(sigma_med_wave)) / sum(np.array(median_values_for_weight))\n",
    "        \n",
    "        return weighted_wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automating_weighted_wave_included_slits(index_of_slit):\n",
    "    \n",
    "    #wavelength_array =  np.arange(4000,11200,200)\n",
    "    wavelength_array = np.arange(4000,11500,500)\n",
    "    #wavelength_array = np.arange(4000,11350,350)\n",
    "    \n",
    "    weighted_wavelength_list = []\n",
    "\n",
    "    for index in range(len(wavelength_array)):\n",
    "\n",
    "        if (index + 1) == len(wavelength_array): \n",
    "            break\n",
    "\n",
    "        else:\n",
    "        \n",
    "            weighted_wavelength = weighted_wave(median, threshold_median,threshold_sky_sub, rbwave_fits[0],wavelength_array[index],\n",
    "                                          wavelength_array[index+1],rbflux_fits,index_of_slit)\n",
    "            \n",
    "            weighted_wavelength_list.append(weighted_wavelength)\n",
    "\n",
    "    #weighted_wavelength_list_final = []\n",
    "    \n",
    "    #for wavelength in weighted_wavelength_list: #filter out any None values \n",
    "        \n",
    "        #if wavelength != None:\n",
    "            \n",
    "            #weighted_wavelength_list_final.append(wavelength)\n",
    "        \n",
    "    return weighted_wavelength_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelength_scaling_function(scaling_value_dict, weighted_wavelength): #used to make a plot of optimal scale factor vs wavelength\n",
    "    \n",
    "    scaling_values = [] #do not plot any scaling value that has None\n",
    "    \n",
    "    wavelength_values = [] #contains all the wavelength we will plots \n",
    "    \n",
    "    for index in range(len(scaling_value_dict.values())): #filtering out the None values\n",
    "        \n",
    "        if list(scaling_value_dict.values())[index] != None:\n",
    "        \n",
    "            scaling_values.append(list(scaling_value_dict.values())[index])\n",
    "            \n",
    "            wavelength_values.append(weighted_wavelength[index])\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    all_wavelength_values = np.arange(4000,11000,0.65) #used to plot every single values between 4000 and 11000 using our polynomial\n",
    "\n",
    "    #finding the polynomial constant for first order\n",
    "    poly_const_first_deg = np.polyfit(wavelength_values,scaling_values,1)\n",
    "    print(\"First order polynomial: y = {0}x + {1}\".format(poly_const_first_deg[0],poly_const_first_deg[1]))\n",
    "    \n",
    "    #finding the polynomial constant for second order\n",
    "    poly_const_second_deg = np.polyfit(wavelength_values,scaling_values,2)\n",
    "    print(\"Second order polynomial: y = {0}x^2 + {1}x + {2}\".format(poly_const_second_deg[0],\n",
    "                                                                    poly_const_second_deg[1],poly_const_second_deg[2]))\n",
    "    \n",
    "    \n",
    "    #finding the polynomial constant for third order\n",
    "    poly_const_third_deg = np.polyfit(wavelength_values,scaling_values,3)\n",
    "    print(\"Third order polynomial: y = {0}(x^3) + {1}(x^2) + {2}(x) + {3}\".format(poly_const_third_deg[0],\n",
    "                                                                                  poly_const_third_deg[1],poly_const_third_deg[2],poly_const_third_deg[3]))\n",
    "    \n",
    "    #finding the polynomial constant for fourth order\n",
    "    poly_const_fourth_deg = np.polyfit(wavelength_values,scaling_values,4)\n",
    "    print(\"Fourth order polynomial: y = {0}(x^4) + {1}(x^3) + {2}(x^2) + {3}(x) + {4}\".format(poly_const_fourth_deg[0],\n",
    "                                                                                  poly_const_fourth_deg[1],poly_const_fourth_deg[2],poly_const_fourth_deg[3]\n",
    "                                                                                             ,poly_const_fourth_deg[4]))\n",
    "    \n",
    "    \n",
    "    #calculating the second, third,and fourth order polynomial as a function of wavelength\n",
    "    poly_first_order = [] #purple line\n",
    "    \n",
    "    poly_second_order = [] #green line\n",
    "    \n",
    "    poly_third_order = [] #blue line\n",
    "    \n",
    "    poly_fourth_order = [] #orange line\n",
    "    \n",
    "    for value in all_wavelength_values:\n",
    "        poly_first_order.append(polynomial_first_order(value,poly_const_first_deg))\n",
    "        \n",
    "        poly_second_order.append(polynomial_second_order(value,poly_const_second_deg))\n",
    "        \n",
    "        poly_third_order.append(polynomial_third_order(value,poly_const_third_deg))\n",
    "        \n",
    "        poly_fourth_order.append(polynomial_fourth_order(value,poly_const_fourth_deg))\n",
    "                                                                                            \n",
    "    \n",
    "    #plotting the scaling factors, third-order polynomial, fourth-order polynomial as a function of wavelength\n",
    "    fig,axs = plt.subplots(1)\n",
    "    axs.set_xlim(3900,11100)\n",
    "    axs.set_ylim(-0.5,2.5)\n",
    "    axs.set_title(\"Optimal Scale Factor vs Wavelength\")\n",
    "    axs.set_xlabel(\"Wavelength (A)\")\n",
    "    axs.set_ylabel(\"Optimal Scale Factor\")\n",
    "    axs.plot(all_wavelength_values,poly_first_order,scalex=False,scaley=False,label=\"First-Order\", c=\"purple\")\n",
    "    axs.plot(all_wavelength_values,poly_second_order,scalex=False,scaley=False,label=\"Second-Order\",c=\"green\")\n",
    "    axs.plot(all_wavelength_values,poly_third_order,scalex=False,scaley=False,label=\"Third-Order\")\n",
    "    axs.plot(all_wavelength_values,poly_fourth_order,scalex=False,scaley=False,label=\"Fourth-Order\")\n",
    "    axs.scatter(wavelength_values,scaling_values,label=\"Optimal Scale Factor\",c=\"black\")\n",
    "    axs.legend()\n",
    "    \n",
    "    return scaling_values,wavelength_values,poly_const_first_deg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving The Polynomial Fits (Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_sigma_clip_scaling(scaling_values_list,weighted_wavelength_list,poly_const_first_deg):\n",
    "    \n",
    "    #optimal shifting value - optimal shifting value based on line of best fit\n",
    "    deviation = []\n",
    "    \n",
    "    #determine the vertical difference (deviation) between dots and line of best fit\n",
    "    for n in range(len(weighted_wavelength_list)):\n",
    "        deviation.append(np.abs(polynomial_first_order(weighted_wavelength_list[n],poly_const_first_deg) - scaling_values_list[n]))\n",
    "        \n",
    "    print(\"First Deviation: {}\".format(deviation))\n",
    "    print(\"First Deviation RMS: {}\".format(statistics.stdev(deviation)))\n",
    "    \n",
    "    \n",
    "    #BEGIN FIRST ITERATION OF SIGMA-CLIPPING\n",
    "    \n",
    "    #variables to store scaling factor and wavelength from first iteration \n",
    "    scaling_values_1 = [] #scaling factor that is not an outlier\n",
    "    wavelength_values_1 = [] #wavelength that is not an outlier\n",
    "    \n",
    "    #variables to store outliers \n",
    "    outlier_deviation = []\n",
    "    \n",
    "    #remove outliers from data\n",
    "    for n in range(len(deviation)):\n",
    "        if deviation[n]/statistics.stdev(deviation) < 3:\n",
    "            scaling_values_1.append(scaling_values_list[n])#keep all non-outlier\n",
    "            wavelength_values_1.append(weighted_wavelength_list[n])\n",
    "        #add all outliers to a separate list\n",
    "        else: \n",
    "            outlier_deviation.append(deviation[n])\n",
    "    \n",
    "    #if there's no outlier, return inputs \n",
    "    if len(outlier_deviation) == 0: \n",
    "        return scaling_values_1, wavelength_values_1\n",
    "        exit()\n",
    "    \n",
    "    #if there are outliers, remove the largest one and keep the remaining outliers\n",
    "    elif len(outlier_deviation) > 1: \n",
    "        outlier_deviation.remove(max(outlier_deviation))\n",
    "        for value in outlier_deviation:\n",
    "            scaling_values_1.append(scaling_values_list[deviation.index(value)])\n",
    "            wavelength_values_1.append(weighted_wavelength_list[deviation.index(value)])\n",
    "     \n",
    "    #if there is only one values left after the outliers is removed, return scaling values and wavelength\n",
    "    if len(scaling_values_1) == 0 or len(scaling_values_1) == 1:\n",
    "        return scaling_values_1,wavelength_values_1\n",
    "        exit()\n",
    "    \n",
    "    \n",
    "    #PERFORM A NEW FITTING\n",
    "    \n",
    "    #calculate new polynomial coefficients\n",
    "    new_poly_coeff = np.polyfit(wavelength_values_1,scaling_values_1,1)\n",
    "    \n",
    "    #CALCULATE NEW DEVIATION AND REMOVE OUTLIERS\n",
    "    \n",
    "    new_deviation = []\n",
    "    \n",
    "    for n in range(len(wavelength_values_1)):\n",
    "        new_deviation.append(np.abs(polynomial_first_order(wavelength_values_1[n],new_poly_coeff) - scaling_values_1[n]))\n",
    "    \n",
    "    print(\"Second Deviation: {}\".format(new_deviation))\n",
    "    print(\"Second Deviation RMS: {}\".format(statistics.stdev(new_deviation)))\n",
    "                             \n",
    "    #final set of data with outliers removed\n",
    "    no_outlier_scaling_values_list = []\n",
    "    no_outlier_wavelength_values_list = []\n",
    "    \n",
    "    #to stores outlier deviation for second iteration\n",
    "    new_outlier_deviation = []\n",
    "    \n",
    "    #remove outliers\n",
    "    for n in range(len(new_deviation)):\n",
    "        if new_deviation[n]/statistics.stdev(new_deviation) < 3:\n",
    "            no_outlier_scaling_values_list.append(scaling_values_1[n])#keep all non-outlier\n",
    "            no_outlier_wavelength_values_list.append(wavelength_values_1[n])\n",
    "        else:\n",
    "            new_outlier_deviation.append(new_deviation[n])\n",
    "\n",
    "    #if there's no outlier, return inputs \n",
    "    if len(new_outlier_deviation) == 0 or len(new_outlier_deviation) == 1: \n",
    "        return no_outlier_scaling_values_list,no_outlier_wavelength_values_list\n",
    "        exit()\n",
    "        \n",
    "    #if there are outliers, remove the largest one and keep the remaining outliers\n",
    "    elif len(new_outlier_deviation) > 1: \n",
    "        new_outlier_deviation.remove(max(new_outlier_deviation))\n",
    "        for value in new_outlier_deviation:\n",
    "            no_outlier_scaling_values_list.append(scaling_values_1[new_deviation.index(value)])\n",
    "            no_outlier_wavelength_values_list.append(wavelength_values_1[new_deviation.index(value)])\n",
    "        return no_outlier_scaling_values_list,no_outlier_wavelength_values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_polynomial_fits(mask_name,slit_num,scaling_values_list,wavelength_values_list):\n",
    "    \n",
    "    no_outlier_scaling_values_list = scaling_values_list #scaling values with outliers removed \n",
    "    no_outlier_wavelength_values_list = wavelength_values_list #wavelength values with outliers removed\n",
    "    \n",
    "    poly_const_first_deg = np.polyfit(no_outlier_wavelength_values_list,no_outlier_scaling_values_list,1) #find the polynomial constants\n",
    "    print(\"New First Order Polynomial: y = {0}x + {1}\".format(poly_const_first_deg[0],\n",
    "                                                                    poly_const_first_deg[1]))\n",
    "    \n",
    "    all_wavelength_values = np.arange(4000,11000,0.65) #used to plot every single values between 4000 and 11000 using our polynomial\n",
    "    \n",
    "    #calculate the new second order polynomial fits\n",
    "    \n",
    "    poly_first_order = [] \n",
    "    \n",
    "    for value in all_wavelength_values:\n",
    "        poly_first_order.append(polynomial_first_order(value,poly_const_first_deg))\n",
    "        \n",
    "        \n",
    "        \n",
    "    #straighten the ends\n",
    "        \n",
    "    #Optimal Scaling Factor of far left wavelength\n",
    "    left_end_val = polynomial_first_order(min(no_outlier_wavelength_values_list),poly_const_first_deg) \n",
    "    \n",
    "    #Optimal Scaling Factor of far right wavelength\n",
    "    right_end_val = polynomial_first_order(max(no_outlier_wavelength_values_list),poly_const_first_deg) \n",
    "    \n",
    "    print(\"Optimal Scale Factor of All Wavelength Before {0} Angstroms: {1}\".format(min(no_outlier_wavelength_values_list),left_end_val)) #print them out \n",
    "    \n",
    "    print(\"Optimal Scale Factor of All Wavelength After {0} Angstroms: {1} \".format(max(no_outlier_wavelength_values_list),right_end_val))\n",
    "    \n",
    "    #straighten the LEFT side of the polynomial fit\n",
    "    for n in range(len(all_wavelength_values)):\n",
    "        if all_wavelength_values[n] < min(no_outlier_wavelength_values_list):\n",
    "            poly_first_order[n] = left_end_val\n",
    "        \n",
    "    #straighten the RIGHT side of the polynomial fit\n",
    "    for n in range(len(all_wavelength_values)):\n",
    "        if all_wavelength_values[n] > max(no_outlier_wavelength_values_list):\n",
    "            poly_first_order[n] = right_end_val\n",
    "    \n",
    "    \n",
    "    #plot\n",
    "    fig,axs = plt.subplots(1)\n",
    "    fig.set_size_inches(8,6)\n",
    "    fig.patch.set_alpha(1)\n",
    "    axs.set_xlim(3900,11100)\n",
    "    axs.set_ylim(-0.5,2.5)\n",
    "    axs.set_title(\"Mask {0}: Slit #{1}\\nOptimal Scale Factor vs Wavelength\".format(mask_name,slit_num))\n",
    "    axs.set_xlabel(\"Wavelength (A)\")\n",
    "    axs.set_ylabel(\"Optimal Scale Factor\")\n",
    "    axs.plot(all_wavelength_values,poly_first_order,scalex=False,scaley=False,label=\"Second-Order\",c=\"green\")\n",
    "    axs.scatter(wavelength_values_list,scaling_values_list,label=\"Optimal Scale Factor\",c=\"black\")\n",
    "    fig.savefig(\"{0}_Polynomial_Graph/Scaling_Fitting/Poly_Graph_{0}_slit_{1}.png\".format(mask_name,slit_num))\n",
    "    \n",
    "    return no_outlier_wavelength_values_list,left_end_val,right_end_val,poly_const_first_deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Original Flux Using Polynomial Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_new_scaling_factor(flux_include,wave_include,no_outlier_wavelength_values_list,left_end_val,right_end_val,poly_const_first_deg):\n",
    "    \n",
    "    flux = flux_include #original flux from FITS files \n",
    "    wave = wave_include #original wavelength from FITS files\n",
    "    \n",
    "    no_outlier_wavelength_values_list = no_outlier_wavelength_values_list #weighted wavelength with outliers removed \n",
    "    \n",
    "    original_flux_scale = []\n",
    "    \n",
    "    for n in range(len(wave_include)): \n",
    "        if wave_include[n] < min(no_outlier_wavelength_values_list):\n",
    "            original_flux_scale.append(flux_include[n] * left_end_val)\n",
    "        \n",
    "        elif wave_include[n] > max(no_outlier_wavelength_values_list): \n",
    "            original_flux_scale.append(flux_include[n] * right_end_val)\n",
    "                                      \n",
    "        else:\n",
    "            scale_factor = polynomial_first_order(wave_include[n],poly_const_first_deg)\n",
    "            original_flux_scale.append(flux_include[n] * scale_factor)\n",
    "            \n",
    "    return original_flux_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Scaled Flux as FITS Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_polynomial_fits_values_scale(no_outlier_scaling_values_list, no_outlier_wavelength_values_list, original_flux_scale, mask_name,slit_number_used):\n",
    "            \n",
    "    hdu1 = fits.PrimaryHDU() #primary HDU (empty)\n",
    "        \n",
    "    c1 = fits.Column(name='SCALE_VALUES', array=no_outlier_scaling_values_list, format='E')\n",
    "    c2 = fits.Column(name='WAVELENGHT_VALUES', array=no_outlier_wavelength_values_list, format='E')\n",
    "    c3 = fits.Column(name='ORIGINAL_FLUX_SCALED', array=original_flux_scale, format='E')\n",
    "\n",
    "    hdu2 = fits.BinTableHDU.from_columns([c1, c2, c3]) #first extensional HDU (w data)\n",
    "            \n",
    "    hdul = fits.HDUList([hdu1, hdu2]) #combine both HDUs into file and write it below\n",
    "        \n",
    "    hdul.writeto(path_name + '/{0}_Rebinned/{0}_Scale_Values/Scale_Values_Polynomial_Fits_{0}_{1}.fits.gz'.format(mask_name,slit_number_used))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shifting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifting_wavelength(waves, shifting_value):\n",
    "    \n",
    "    waves_shifted = []\n",
    "    \n",
    "    for i in range(len(waves)):\n",
    "        waves_shifted.append(waves[i] + shifting_value)\n",
    "    \n",
    "    return waves_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looping_shifting_wavelength(original_wavelength,test_values,index_of_slit):\n",
    "    \n",
    "    wave_shifted_dict = {}\n",
    "    \n",
    "    for value in test_values: \n",
    "        wave_shifted = shifting_wavelength(original_wavelength[index_of_slit],value)\n",
    "        wave_shifted_dict[\"Shifted_{}\".format(round(value,2))] = wave_shifted\n",
    "    \n",
    "    return wave_shifted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebin_wave_shifted(wave_shifted_dict,flux,ivar,test_values,index_of_slit):\n",
    "    \n",
    "    rbflux_shifted_dict = {}\n",
    "    \n",
    "    for value in test_values:\n",
    "        \n",
    "        rbflux_shifted,rbwave_shifted,rbivar_shifted = rebin([flux[index_of_slit]], \n",
    "                                                             [wave_shifted_dict[\"Shifted_{}\".format(round(value,2))]], \n",
    "                                                             [ivar[index_of_slit]], 600)\n",
    "        \n",
    "        \n",
    "        rbflux_shifted_dict[\"Shifted_{}\".format(round(value,2))] = rbflux_shifted\n",
    "    \n",
    "    return rbflux_shifted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def sorting_shifted_rms(multiply_boolean,rbflux_shifted_dict,shift_test_values):\n",
    "    #rms_dict_sorted = {}\n",
    "    \n",
    "    #for test_value in shift_test_values:\n",
    "        \n",
    "     #   rbflux_shifted = rbflux_shifted_dict[\"Shifted_{}\".format(round(test_value,2))]\n",
    "\n",
    "      #  rms_dict_sorted[\"Shifted_{}\".format(round(test_value,2))] = rbflux_shifted[0][np.where(multiply_boolean == True)]\n",
    "                    \n",
    "    #return rms_dict_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_shifted_rms(multiply_boolean,rbflux_shifted_dict,shift_test_values):\n",
    "    rms_dict_sorted = {}\n",
    "    \n",
    "    for test_value in shift_test_values:\n",
    "        \n",
    "        #rms_values = []\n",
    "        \n",
    "        rbflux_shifted = rbflux_shifted_dict[\"Shifted_{}\".format(round(test_value,2))]\n",
    "        \n",
    "        #for n in range(len(multiply_boolean)):\n",
    "\n",
    "            #if multiply_boolean[n] == True: \n",
    "\n",
    "              #  rms_values.append(rbflux_shifted[0][n])\n",
    "\n",
    "            #else:\n",
    "\n",
    "               # pass\n",
    "        #   \n",
    "        rms_values = np.array(rbflux_shifted[0])[np.where(multiply_boolean == True)]\n",
    "        \n",
    "        rms_dict_sorted[\"Shifted_{}\".format(round(test_value,2))] = rms_values\n",
    "                    \n",
    "    return rms_dict_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbflux_shifted_minus_median(multiply_boolean,scaling_value_dict,rms_dict_sorted,shift_test_values,min_wave,max_wave):\n",
    "    if scaling_value_dict[\"{0}_to_{1}\".format(min_wave,max_wave)] != None:\n",
    "        optimal_scaling_factor = scaling_value_dict[\"{0}_to_{1}\".format(min_wave,max_wave)]\n",
    "    else:\n",
    "        optimal_scaling_factor = 1\n",
    "        \n",
    "    subtraction_dict_shifted = {}\n",
    "    \n",
    "    median_cut = np.array(median)[np.where(multiply_boolean == True)]\n",
    "    \n",
    "    for test_value in shift_test_values:\n",
    "        \n",
    "        rms_values_sorted = rms_dict_sorted[\"Shifted_{0}\".format(round(test_value,2))]\n",
    "        \n",
    "        #\n",
    "        subtraction_list = (np.array(rms_values_sorted)*optimal_scaling_factor) - median_cut\n",
    "        \n",
    "       # for n in range(len(rms_values_sorted)):\n",
    "                \n",
    "                #subtraction = (rms_values_sorted[n]*optimal_scaling_factor) - median_cut[n]\n",
    "                #subtraction = rbflux_shifted[n]- median[n]\n",
    "                \n",
    "                #subtraction_list.append(subtraction)\n",
    "                \n",
    "                \n",
    "        subtraction_dict_shifted[\"Shifted_{0}\".format(round(test_value,2))] = subtraction_list \n",
    "    \n",
    "    return subtraction_dict_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_calculation_shift(rms_dict_sorted_shift, shift_test_values):\n",
    "    \n",
    "    rms_dict_shift = {}\n",
    "\n",
    "    for test_value in shift_test_values:\n",
    "\n",
    "        values_for_rms_cal = rms_dict_sorted_shift[\"Shifted_{}\".format(round(test_value,2))]\n",
    "\n",
    "        rms = statistics.stdev(values_for_rms_cal)\n",
    "\n",
    "        rms_dict_shift[\"Shifted_{}\".format(round(test_value,2))] = rms\n",
    "        \n",
    "    return rms_dict_shift\n",
    "    \n",
    "    \n",
    "        #print(\"Everything is False. There's no True boolean. Therefore, RMS cannot be calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_the_shift_rms(slit_number,shift_test_values, rms_dict_shift, min_wave, max_wave):\n",
    "    \n",
    "    value_list = []\n",
    "    path = \"./{0}_Polynomial_Graph/Scaling_vs_RMS(Shifting)/Slit_{1}\".format(mask_name,slit_number)\n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise\n",
    "    for test_value in shift_test_values: \n",
    "\n",
    "        value_list.append(rms_dict_shift[\"Shifted_{0}\".format(round(test_value,2))])\n",
    "\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    fig.patch.set_alpha(1)\n",
    "    plt.plot(shift_test_values,value_list)\n",
    "    plt.xlabel(\"Scaling\")\n",
    "    plt.ylabel(\"RMS\")\n",
    "    plt.title(\"Mask {0}: Slit #{1}\\nRMS vs Scaling (Shifting Process) ({2} A to {3} A)\".format(mask_name,slit_number,min_wave,max_wave))\n",
    "    fig.savefig(path+'/{0}_Slit_{1}_{2}_to_{3}.png'.format(mask_name,slit_number,min_wave,max_wave))\n",
    "        \n",
    "    min_val = min(value_list)\n",
    "\n",
    "    shifting_value = round(shift_test_values[value_list.index(min_val)],2)\n",
    "\n",
    "    print(\"Shifting w/ minimum RMS (Slit #{0}): {1}\".format(slit_number,shifting_value) + \" ({0} A to {1} A)\".format(min_wave,max_wave))\n",
    "\n",
    "    return shifting_value\n",
    "        \n",
    "    \n",
    "        #print(\"Because we have no RMS there is no plot.\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_with_shifting(median,rbflux,multiplier,min_ylim,max_ylim,min_xlim,max_xlim):\n",
    "    new_flux = [] #sky subtracted spectra w/ scaling\n",
    "    new_flux_no_scaling = [] #sky subtracted spectra w/o scaling\n",
    "    rbflux_with_scaling = []\n",
    "    spectrum = rbflux\n",
    "    multiplier = multiplier \n",
    "\n",
    "    #multiplying the rbflux by scaling factor \n",
    "    for i in range(len(spectrum)):\n",
    "        if np.isfinite(spectrum[i]) == True:\n",
    "            new_flux.append((spectrum[i] * multiplier) - median[i]) \n",
    "            rbflux_with_scaling.append((spectrum[i] * multiplier))\n",
    "        else:\n",
    "            new_flux.append(spectrum[i])\n",
    "            rbflux_with_scaling.append(spectrum[i])\n",
    "\n",
    "     #no scaling factor\n",
    "    for i in range(len(spectrum)):\n",
    "        if np.isfinite(spectrum[i]) == True:\n",
    "            new_flux_no_scaling.append((spectrum[i]) - median[i]) \n",
    "        else:\n",
    "            new_flux_no_scaling.append(spectrum[i])\n",
    "        \n",
    "    #plotting \n",
    "    fig,axs = plt.subplots(1)\n",
    "    plt.ylim(min_ylim,max_ylim) #could try getting a smaller y limit and getting rid of legend\n",
    "    plt.xlim(min_xlim,max_xlim)\n",
    "    plt.plot(new_wave_600, median, c=\"r\", scalex=False, scaley=False, label = \"median\")\n",
    "    plt.plot(new_wave_600, rbflux_with_scaling, c=\"b\", scalex=False, scaley=False, label = \"rebinned w/ scaling\")\n",
    "    plt.plot(new_wave_600, np.array(new_flux_no_scaling), c=\"purple\", scalex=False, scaley=False, label = \"Subtracted w/o scaling\")\n",
    "    plt.plot(new_wave_600, np.array(new_flux), c=\"g\", scalex=False, scaley=False,label = \"subtracted\")\n",
    "    plt.xlabel(\"Wavelength (A)\")\n",
    "    plt.ylabel(\"Flux (Electron/Hour)\")\n",
    "    plt.title(\"Slit #115 Scaled + Shifted\" + \"/Multiplier: {}\".format(multiplier) + \"/From {0} to {1}\".format(min_xlim,max_xlim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating All The Functions Used in The Shifting Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_shifting(slit_number,flux,wave,ivar,scaling_value_dict,median,threshold_median,wavelength,\n",
    "                     slit_index,rbflux_list,threshold_sky_sub):\n",
    "    \n",
    "    #define all values we want to test\n",
    "    shift_test_values = np.arange(-0.2,0.21,0.01)\n",
    "    \n",
    "    #shift original wavelength by test values\n",
    "    #store everything as a dictionary. Format: \"Shifted_(test_value):[shifted wavelength]\"\n",
    "    wave_shifted_dict = looping_shifting_wavelength(wave,shift_test_values,slit_index) \n",
    "    \n",
    "    #rebin using shifted wavelength \n",
    "    #store rebinned flux as a dictionary. Format: \" Shifted_(test_value):[rebinned flux]\"\n",
    "    rbflux_shifted_dict = rebin_wave_shifted(wave_shifted_dict,flux,ivar,shift_test_values,slit_index)\n",
    "    \n",
    "    #500 A segments \n",
    "    wavelength_array = np.arange(4000,11500,500)\n",
    "    \n",
    "    shifting_value_dict = {}\n",
    "    \n",
    "    #for loop to find the multiply_boolean and median-subtracted spectrum\n",
    "    for n in range(len(wavelength_array)):\n",
    "\n",
    "        if (n + 1) == len(wavelength_array): \n",
    "            break\n",
    "\n",
    "        else:\n",
    "            #boolean array using median and threshold\n",
    "            median_boolean_array = median_threshold(median,threshold_median)\n",
    "    \n",
    "            #boolean array using wavelength\n",
    "            wavelength_boolean_array = create_wave_bool(wavelength, wavelength_array[n], wavelength_array[n+1])\n",
    "\n",
    "            #boolean array using rbflux - median \n",
    "            sky_sub_boolean_array = sky_sub_bool(slit_index,rbflux_list,median,threshold_sky_sub)\n",
    "\n",
    "            #multiply the two boolean arrays\n",
    "            multiply_boolean = median_boolean_array * wavelength_boolean_array * sky_sub_boolean_array\n",
    "\n",
    "            #values that will be used to calculate the rms \n",
    "            values_for_shift_rms = sorting_shifted_rms(multiply_boolean,rbflux_shifted_dict,shift_test_values)\n",
    "            \n",
    "            #use the rebinned flux, the optimal scaling factor, and median to calculate the median-subtracted spectrum\n",
    "            #scaling_value_dict contain the optimal scaling factor of each 500 segment\n",
    "            subtraction_dict_shifted = rbflux_shifted_minus_median(multiply_boolean,scaling_value_dict,values_for_shift_rms,\n",
    "                                                                   shift_test_values,wavelength_array[n],wavelength_array[n+1])\n",
    "            \n",
    "            \n",
    "            if len(subtraction_dict_shifted[\"Shifted_0.2\"]) == 0 or len(subtraction_dict_shifted[\"Shifted_0.2\"]) == 1:\n",
    "                shifting_value_dict[\"{0}_to_{1}\".format(wavelength_array[n],wavelength_array[n+1])] = None\n",
    "                print(\"Boolean are all False. No values can be use to calculate the RMS. From {0} to {1}\".format(wavelength_array[n],wavelength_array[n+1]))\n",
    "                \n",
    "            else: \n",
    "            \n",
    "                #rms values\n",
    "                rms_dict_shift = rms_calculation_shift(subtraction_dict_shifted,shift_test_values)\n",
    "\n",
    "                #rms values that will be used \n",
    "                shifting_value = plotting_the_shift_rms(slit_number,shift_test_values,rms_dict_shift,wavelength_array[n],wavelength_array[n+1])\n",
    "\n",
    "                shifting_value_dict[\"{0}_to_{1}\".format(wavelength_array[n],wavelength_array[n+1])] = shifting_value\n",
    "            \n",
    "    return shifting_value_dict,rbflux_shifted_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Scale and Shift Factor and Reading It Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_scaling_and_shifting_factor(mask_name,slit_number_used,scaling_value_dict,shifting_value_dict):\n",
    "    \n",
    "    hdu1 = fits.PrimaryHDU()\n",
    "    \n",
    "    c1 = fits.Column(name=\"SCALING_FACTOR\", array=list(scaling_value_dict.values()), format='E')\n",
    "    c2 = fits.Column(name=\"SHIFTING_FACTOR\", array=list(shifting_value_dict.values()), format='E')\n",
    "\n",
    "    hdu2 = fits.BinTableHDU.from_columns([c1,c2])\n",
    "    \n",
    "    hdul = fits.HDUList([hdu1,hdu2])\n",
    "    \n",
    "    hdul.writeto(path_name + '/{0}_Rebinned/{0}_Scaling_and_Shifting_Factor/Scaling_and_Shifting_Factor_{0}_{1}.fits.gz'.format(mask_name,slit_number_used))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Rebinned Flux Calculated Using Shifted Wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving rbflux_shifted_dict as a nyp files, take a bit of space, need to reduce it.\n",
    "#get only the rbflux that we will need \n",
    "def sorting_needed_shift_factor(shifting_value_dict,rbflux_shifted_dict):\n",
    "    \n",
    "    original_shift_factor_list = list(shifting_value_dict.values())\n",
    "    \n",
    "    #remove all duplicate and all None \n",
    "    remove_duplicate_and_none = list(set(original_shift_factor_list))\n",
    "    \n",
    "    #selects only values that are optimal shift factor\n",
    "    sorted_rbflux_shifted_dict = {}\n",
    "    \n",
    "    for value in remove_duplicate_and_none:\n",
    "        if value == None:\n",
    "            remove_duplicate_and_none.remove(value)\n",
    "            \n",
    "    for value in remove_duplicate_and_none:\n",
    "        sorted_rbflux_shifted_dict[\"Shifted_{0}\".format(value)] = rbflux_shifted_dict[\"Shifted_{0}\".format(value)]\n",
    "            \n",
    "    return(sorted_rbflux_shifted_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note sure how to save dictionary as FITS file, therefore, use npy file\n",
    "def saving_rbflux_shifted(mask_name,slit_number_used,sorted_rbflux_shifted_dict):\n",
    "    \n",
    "    #saving the dictionary as a npy file\n",
    "    np.save(\"./{0}_Rebinned/{0}_Rebinned_Flux_Shifted_Wave/rbflux_shifted_dict_{0}_{1}.npy\".format(mask_name,slit_number_used),sorted_rbflux_shifted_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Fit for Shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelength_shifting_function(shifting_value_dict, weighted_wavelength): #used to make a plot of optimal shifting factor vs wavelength\n",
    "    \n",
    "    shifting_values = [] #do not plot any shifting value that has None\n",
    "    \n",
    "    wavelength_values = [] #contains all the wavelength we will plots \n",
    "    \n",
    "    for index in range(len(shifting_value_dict.values())): #filtering out the None values\n",
    "        \n",
    "        if list(shifting_value_dict.values())[index] != None:\n",
    "        \n",
    "            shifting_values.append(list(shifting_value_dict.values())[index])\n",
    "            \n",
    "            wavelength_values.append(weighted_wavelength[index])\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    all_wavelength_values = np.arange(4000,11000,0.65) #used to plot every single values between 4000 and 11000 using our polynomial\n",
    "\n",
    "    #finding the polynomial constant for second order\n",
    "    poly_const_second_deg = np.polyfit(wavelength_values,shifting_values,2)\n",
    "    print(\"Second order polynomial: y = {0}x^2 + {1}x + {2}\".format(poly_const_second_deg[0],\n",
    "                                                                    poly_const_second_deg[1],poly_const_second_deg[2]))\n",
    "    \n",
    "    #finding the polynomial constant for third order\n",
    "    poly_const_third_deg = np.polyfit(wavelength_values,shifting_values,3)\n",
    "    print(\"Third order polynomial: y = {0}(x^3) + {1}(x^2) + {2}(x) + {3}\".format(poly_const_third_deg[0],\n",
    "                                                                                  poly_const_third_deg[1],poly_const_third_deg[2],poly_const_third_deg[3]))\n",
    "    \n",
    "    #finding the polynomial constant for fourth order\n",
    "    poly_const_fourth_deg = np.polyfit(wavelength_values,shifting_values,4)\n",
    "    print(\"Fourth order polynomial: y = {0}(x^4) + {1}(x^3) + {2}(x^2) + {3}(x) + {4}\".format(poly_const_fourth_deg[0],\n",
    "                                                                                  poly_const_fourth_deg[1],poly_const_fourth_deg[2],poly_const_fourth_deg[3]\n",
    "                                                                                             ,poly_const_fourth_deg[4]))\n",
    "    \n",
    "    \n",
    "    #calculating the third and fourth order polynomial as a function of wavelength\n",
    "    poly_second_order = []\n",
    "    \n",
    "    poly_third_order = []\n",
    "    \n",
    "    poly_fourth_order = []\n",
    "    \n",
    "    for value in all_wavelength_values:\n",
    "        poly_second_order.append(polynomial_second_order(value,poly_const_second_deg))\n",
    "        \n",
    "        poly_third_order.append(polynomial_third_order(value,poly_const_third_deg))\n",
    "        \n",
    "        poly_fourth_order.append(polynomial_fourth_order(value,poly_const_fourth_deg))\n",
    "                                                                                            \n",
    "    #plotting the optimal shifting factors, third-order polynomial, fourth-order polynomial as a function of wavelength\n",
    "    fig,axs = plt.subplots(1)\n",
    "    axs.set_xlim(3900,11100)\n",
    "    axs.set_ylim(-0.25,0.25)\n",
    "    axs.set_title(\"Optimal Shifting Factor vs Wavelength\")\n",
    "    axs.set_xlabel(\"Wavelength (A)\")\n",
    "    axs.set_ylabel(\"Optimal Shifting Factor\")\n",
    "    axs.plot(all_wavelength_values,poly_second_order,scalex=False,scaley=False,label=\"Second-Order\",c=\"green\")\n",
    "    axs.plot(all_wavelength_values,poly_third_order,scalex=False,scaley=False,label=\"Third-Order\",c=\"blue\")\n",
    "    axs.plot(all_wavelength_values,poly_fourth_order,scalex=False,scaley=False,label=\"Fourth-Order\",c=\"orange\")\n",
    "    axs.scatter(wavelength_values, shifting_values,label=\"Optimal Shifting Factor\",c=\"black\")\n",
    "    axs.legend()\n",
    "    \n",
    "    return shifting_values,wavelength_values,poly_const_second_deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving The Polymial Fits (Shifting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_sigma_clip_shifting(shifting_values_list,weighted_wavelength_list,poly_const_second_deg):\n",
    "    \n",
    "    #optimal shifting value - optimal shifting value based on line of best fit\n",
    "    deviation = [] \n",
    "    \n",
    "    #determine the vertical difference (deviation) between dots and line of best fit\n",
    "    for n in range(len(weighted_wavelength_list)):\n",
    "        deviation.append(np.abs(polynomial_second_order(weighted_wavelength_list[n],poly_const_second_deg) - shifting_values_list[n]))\n",
    "    \n",
    "    print(\"First Deviation: {}\".format(deviation))\n",
    "    print(\"First Deviation RMS: {}\".format(statistics.stdev(deviation)))\n",
    "    \n",
    "    #BEGIN FIRST ITERATION\n",
    "    \n",
    "    #first iteration \n",
    "    shifting_values_1 = []\n",
    "    wavelength_values_1 = []\n",
    "    \n",
    "    #to store outlier\n",
    "    outlier_deviation = []\n",
    "    \n",
    "    #remove outliers from data\n",
    "    for n in range(len(deviation)):\n",
    "        if deviation[n]/statistics.stdev(deviation) < 3.5:\n",
    "            shifting_values_1.append(shifting_values_list[n])#keep all non-outlier\n",
    "            wavelength_values_1.append(weighted_wavelength_list[n])\n",
    "        #add all outliers to a separate list\n",
    "        else: \n",
    "            outlier_deviation.append(deviation[n])\n",
    "    \n",
    "    #if there's no outlier, return inputs \n",
    "    if len(outlier_deviation) == 0: \n",
    "        return shifting_values_1, wavelength_values_1\n",
    "        exit()\n",
    "    \n",
    "    #if there are outliers, remove the largest one and keep the remaining outliers\n",
    "    elif len(outlier_deviation) > 1: \n",
    "        outlier_deviation.remove(max(outlier_deviation))\n",
    "        for value in outlier_deviation:\n",
    "            shifting_values_1.append(shifting_values_list[deviation.index(value)])\n",
    "            wavelength_values_1.append(weighted_wavelength_list[deviation.index(value)])\n",
    "     \n",
    "    #if there is only one values left after the outliers is removed, return shifting values and wavelength\n",
    "    if len(shifting_values_1) == 0 or len(shifting_values_1) == 1:\n",
    "        return shifting_values_1,wavelength_values_1\n",
    "        exit()\n",
    "    \n",
    "    #PERFORM A NEW FITTING\n",
    "    \n",
    "    new_poly_coeff = np.polyfit(wavelength_values_1,shifting_values_1,2)\n",
    "    \n",
    "    \n",
    "    #CALCULATE NEW DEVIATION AND REMOVE OUTLIERS\n",
    "    \n",
    "    new_deviation = []\n",
    "    \n",
    "    for n in range(len(wavelength_values_1)):\n",
    "        new_deviation.append(np.abs(polynomial_second_order(wavelength_values_1[n],new_poly_coeff) - shifting_values_1[n]))\n",
    "    \n",
    "    print(\"Second Deviation: {}\".format(new_deviation))\n",
    "    print(\"Second Deviation RMS: {}\".format(statistics.stdev(new_deviation)))\n",
    "                             \n",
    "    #final set of data with outliers removed\n",
    "    no_outlier_shifting_values_list = []\n",
    "    no_outlier_wavelength_values_list = []\n",
    "    \n",
    "    #to stores outliers\n",
    "    new_outlier_deviation = []\n",
    "    \n",
    "    #remove outliers\n",
    "    for n in range(len(new_deviation)):\n",
    "        if new_deviation[n]/statistics.stdev(new_deviation) < 3.5:\n",
    "            no_outlier_shifting_values_list.append(shifting_values_1[n])#keep all non-outlier\n",
    "            no_outlier_wavelength_values_list.append(wavelength_values_1[n])\n",
    "        else:\n",
    "            new_outlier_deviation.append(new_deviation[n])\n",
    "    \n",
    "    #if there's no outlier or just one outlier, return final list of scale factor and wavelength \n",
    "    if len(new_outlier_deviation) == 0 or len(new_outlier_deviation) == 1:\n",
    "        return no_outlier_shifting_values_list, no_outlier_wavelength_values_list\n",
    "        exit()\n",
    "    \n",
    "    #if there are outliers, remove the largest one and keep the remaining outliers\n",
    "    elif len(new_outlier_deviation) > 1: \n",
    "        new_outlier_deviation.remove(max(new_outlier_deviation))\n",
    "        for value in new_outlier_deviation:\n",
    "            no_outlier_shifting_values_list.append(shifting_values_1[new_deviation.index(value)])\n",
    "            no_outlier_wavelength_values_list.append(wavelength_values_1[new_deviation.index(value)])\n",
    "        return no_outlier_shifting_values_list,no_outlier_wavelength_values_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_polynomial_fits_shifting(mask_name,slit_num,shifting_values_list,wavelength_values_list):\n",
    "    \n",
    "    no_outlier_shifting_values_list = shifting_values_list #scaling values with outliers removed \n",
    "    no_outlier_wavelength_values_list = wavelength_values_list #wavelength values with outliers removed\n",
    "    \n",
    "    poly_const_second_deg = np.polyfit(no_outlier_wavelength_values_list,no_outlier_shifting_values_list,2) #find the polynomial constants\n",
    "    print(\"New Second Order Polynomial: y = {0}x^2 + {1}x + {2}\".format(poly_const_second_deg[0],\n",
    "                                                                    poly_const_second_deg[1],poly_const_second_deg[2]))\n",
    "    \n",
    "    all_wavelength_values = np.arange(4000,11000,0.65) #used to plot every single values between 4000 and 11000 using our polynomial\n",
    "    \n",
    "    #calculate the new second order polynomial fits\n",
    "    \n",
    "    poly_second_order = [] \n",
    "    \n",
    "    for value in all_wavelength_values:\n",
    "        poly_second_order.append(polynomial_second_order(value,poly_const_second_deg))\n",
    "        \n",
    "    #straighten the ends\n",
    "        \n",
    "    #Optimal Scaling Factor of far left wavelength\n",
    "    left_end_val = polynomial_second_order(min(no_outlier_wavelength_values_list),poly_const_second_deg) \n",
    "    \n",
    "    #Optimal Scaling Factor of far right wavelength\n",
    "    right_end_val = polynomial_second_order(max(no_outlier_wavelength_values_list),poly_const_second_deg) \n",
    "    \n",
    "    print(\"Optimal Shift Factor of All Wavelength Before {0} Angstroms: {1}\".format(min(no_outlier_wavelength_values_list),left_end_val)) #print them out \n",
    "    \n",
    "    print(\"Optimal Shift Factor of All Wavelength After {0} Angstroms: {1} \".format(max(no_outlier_wavelength_values_list),right_end_val))\n",
    "    \n",
    "    #straighten the LEFT side of the polynomial fit\n",
    "    for n in range(len(all_wavelength_values)):\n",
    "        if all_wavelength_values[n] < min(no_outlier_wavelength_values_list):\n",
    "            poly_second_order[n] = left_end_val\n",
    "        \n",
    "    #straighten the RIGHT side of the polynomial fit\n",
    "    for n in range(len(all_wavelength_values)):\n",
    "        if all_wavelength_values[n] > max(no_outlier_wavelength_values_list):\n",
    "            poly_second_order[n] = right_end_val\n",
    "        \n",
    "    #plot\n",
    "    fig,axs = plt.subplots(1)\n",
    "    fig.set_size_inches(8,6)\n",
    "    fig.patch.set_alpha(1)\n",
    "    axs.set_xlim(3900,11100)\n",
    "    axs.set_ylim(-0.25,0.25)\n",
    "    axs.set_title(\"Mask {0}: Slit #{1}\\nOptimal Shift Factor vs Wavelength\".format(mask_name,slit_num))\n",
    "    axs.set_xlabel(\"Wavelength (A)\")\n",
    "    axs.set_ylabel(\"Optimal Shift Factor\")\n",
    "    axs.plot(all_wavelength_values,poly_second_order,scalex=False,scaley=False,label=\"Second-Order\",c=\"green\")\n",
    "    axs.scatter(wavelength_values_list,shifting_values_list,label=\"Optimal Shift Factor\",c=\"black\")\n",
    "    fig.savefig(\"{0}_Polynomial_Graph/Shifting_Fitting/Poly_Graph_{0}_slit_{1}.png\".format(mask_name,slit_num))\n",
    "    \n",
    "    return no_outlier_wavelength_values_list,left_end_val,right_end_val,poly_const_second_deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift Original Wavelength Using Polynomial Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_new_shifting_factor(wave,no_outlier_wavelength_values_list,left_end_val,right_end_val,poly_const_second_deg):\n",
    "     \n",
    "    wave = wave #original wavelength from FITS files\n",
    "    \n",
    "    no_outlier_wavelength_values_list = no_outlier_wavelength_values_list #weighted wavelength with outliers removed \n",
    "    \n",
    "    original_wavelength_shift = []\n",
    "    \n",
    "    for n in range(len(wave)): \n",
    "        if wave[n] < min(no_outlier_wavelength_values_list):\n",
    "            original_wavelength_shift.append(wave[n] + left_end_val)\n",
    "        \n",
    "        elif wave[n] > max(no_outlier_wavelength_values_list): \n",
    "            original_wavelength_shift.append(wave[n] + right_end_val)\n",
    "                                      \n",
    "        else:\n",
    "            shift_factor = polynomial_second_order(wave[n],poly_const_second_deg)\n",
    "            original_wavelength_shift.append(wave[n] + shift_factor)\n",
    "            \n",
    "    return original_wavelength_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Shifted Wavelength as FITS Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_polynomial_fits_values_shift(no_outlier_scaling_values_list, no_outlier_wavelength_values_list, original_wavelength_shift,mask_name,slit_number_used):\n",
    "            \n",
    "    hdu1 = fits.PrimaryHDU() #primary HDU (empty)\n",
    "        \n",
    "    c1 = fits.Column(name='SHIFT_VALUES', array=no_outlier_scaling_values_list, format='E')\n",
    "    c2 = fits.Column(name='WAVELENGHT_VALUES', array=no_outlier_wavelength_values_list, format='E')\n",
    "    c3 = fits.Column(name='ORIGINAL_WAVELENGTH_SHIFTED', array=original_wavelength_shift, format='E')\n",
    "\n",
    "    hdu2 = fits.BinTableHDU.from_columns([c1, c2, c3]) #first extensional HDU (w data)\n",
    "            \n",
    "    hdul = fits.HDUList([hdu1, hdu2]) #combine both HDUs into file and write it below\n",
    "        \n",
    "    hdul.writeto(path_name + '/{0}_Rebinned/{0}_Shift_Values/Shift_Values_Polynomial_Fits_{0}_{1}.fits.gz'.format(mask_name,slit_number_used))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Polynomial Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_polynomial_coeff(mask_name,slit_number_used,poly_const_first_deg_scal,poly_const_second_deg_shift,left_end_val,right_end_val,no_outlier_wavelength_values_list):\n",
    "    \n",
    "    hdu1 = fits.PrimaryHDU() #primary HDU (empty)\n",
    "        \n",
    "    c1 = fits.Column(name='POLYNOMIAL_COEFFICIENTS_SCALING', array=poly_const_first_deg_scal, format='E')\n",
    "    c2 = fits.Column(name='POLYNOMIAL_COEFFICIENTS_SHIFTING', array=poly_const_second_deg_shift, format='E')\n",
    "    c3 = fits.Column(name='END_VALUES', array=[left_end_val,right_end_val], format='E')\n",
    "    c4 = fits.Column(name='NO_OUTLIER_WEIGHTED_WAVE',array=no_outlier_wavelength_values_list,format='E')\n",
    "    \n",
    "    hdu2 = fits.BinTableHDU.from_columns([c1,c2,c3,c4]) #first extensional HDU (w data)\n",
    "            \n",
    "    hdul = fits.HDUList([hdu1, hdu2]) #combine both HDUs into file and write it below\n",
    "        \n",
    "    hdul.writeto(path_name + '/{0}_Rebinned/{0}_Polynomial_Coefficients/Poly_Coeff_{0}_{1}.fits.gz'.format(mask_name,slit_number_used))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate Optimization of Included Slits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looping_through_all_function_included_slits(slit_numbers_to_automate):\n",
    "    for slit_number in slit_numbers_to_automate:\n",
    "        \n",
    "        slit_number_used = slit_number\n",
    "        print('\\nOptimizing slit #{0} (Included) from mask {1}.'.format(slit_number_used,mask_name))\n",
    "\n",
    "        index_of_slit = slit_nums.index(slit_number_used) #index of slit that are excluded (Contain ISM Em Line)\n",
    "        #slit_nums_exclude will need to be changed for slits that are included\n",
    "\n",
    "        #Determine the Optimal Scale Factor \n",
    "        print('Finding the Optimal Scale Factor...')\n",
    "        subtraction_dict = rbflux_minus_median(rbflux_fits[index_of_slit], median, multipliers)\n",
    "\n",
    "        scaling_value_dict = looping_finding_scale_included_slits(slit_number_used,index_of_slit,subtraction_dict)\n",
    "\n",
    "        #Determine the weighted wavelength \n",
    "        weighted_wavelength_list = automating_weighted_wave_included_slits(index_of_slit) #there is some None values. Let's filter it out! \n",
    "        print(\"List of All Weighted Wavelength: {0}\".format(weighted_wavelength_list))\n",
    "\n",
    "        try:\n",
    "            #Original Polynomial Fits for Scaling\n",
    "            scaling_values_list,wavelength_values_scal,poly_const_first_deg_scaling = wavelength_scaling_function(scaling_value_dict,weighted_wavelength_list)\n",
    "\n",
    "            #Improving Original Polynomial Fits for Scaling\n",
    "            #Remove Outliers Using Sigma Clipping\n",
    "            no_outlier_scaling_values_list, no_outlier_wavelength_values_list_scal = remove_outliers_sigma_clip_scaling(scaling_values_list,wavelength_values_scal,poly_const_first_deg_scaling)\n",
    "\n",
    "            #New Polynomial Fits for Scaling\n",
    "            no_outlier_wavelength_values_list_scal,left_end_val_scal,right_end_val_scal,poly_const_first_deg_scal = new_polynomial_fits(mask_name,slit_number_used,no_outlier_scaling_values_list,no_outlier_wavelength_values_list_scal)\n",
    "\n",
    "            #Use New Poly Fits to Scale Original Flux\n",
    "            original_flux_scale = use_new_scaling_factor(flux[index_of_slit],wave[index_of_slit],\n",
    "                               no_outlier_wavelength_values_list_scal,left_end_val_scal,\n",
    "                               right_end_val_scal,poly_const_first_deg_scal)\n",
    "\n",
    "        except TypeError:\n",
    "            poly_const_first_deg_scal = [0,1] #straight horizontal line, no scaling\n",
    "            no_outlier_scaling_values_list = list(scaling_value_dict.values())\n",
    "            no_outlier_wavelength_values_list_scal = weighted_wavelength_list\n",
    "            original_flux_scale = flux[index_of_slit]\n",
    "            \n",
    "        \n",
    "        #Saving Scaled Original Flux as FITS Files\n",
    "        save_new_polynomial_fits_values_scale(no_outlier_scaling_values_list, no_outlier_wavelength_values_list_scal, original_flux_scale,mask_name,slit_number_used)\n",
    "\n",
    "        \n",
    "        #Determine the Optimal Shift Factor \n",
    "        print('Finding the Optimal Shift Factor...')\n",
    "        shifting_value_dict,rbflux_shifted_dict = finding_shifting(slit_number_used,flux,wave,ivar,scaling_value_dict,median,threshold_median,rbwave_fits[0],index_of_slit,rbflux_fits,threshold_sky_sub)\n",
    "        \n",
    "        #Saving the Scale Factor and Shift Factor\n",
    "        saving_scaling_and_shifting_factor(mask_name,slit_number_used,scaling_value_dict,shifting_value_dict)\n",
    "        \n",
    "        #Sort through rbflux_shifted_dict and remove all key-value we don't need (reduce size of saved file)\n",
    "        sorted_rbflux_shifted_dict = sorting_needed_shift_factor(shifting_value_dict,rbflux_shifted_dict)\n",
    "        \n",
    "        #Saving the rebinned flux calculated using the shifted wavelength\n",
    "        saving_rbflux_shifted(mask_name,slit_number_used,sorted_rbflux_shifted_dict)\n",
    "\n",
    "        try:\n",
    "            #Original Polynomial Fits for Shifting\n",
    "            shifting_values_list,wavelength_values_shift,poly_const_second_deg_shifting = wavelength_shifting_function(shifting_value_dict,weighted_wavelength_list)\n",
    "\n",
    "            #Improving Original Polynomial Fits for Shifting\n",
    "            #Remove Outliers \n",
    "            no_outlier_shifting_values_list, no_outlier_wavelength_values_list_shift = remove_outliers_sigma_clip_shifting(shifting_values_list,wavelength_values_shift,poly_const_second_deg_shifting)\n",
    "\n",
    "            #New Polynomial Fits for Shifting\n",
    "            no_outlier_wavelength_values_list_shift,left_end_val_shift,right_end_val_shift,poly_const_second_deg_shift = new_polynomial_fits_shifting(mask_name,slit_number_used,no_outlier_shifting_values_list,no_outlier_wavelength_values_list_shift)\n",
    "\n",
    "            #Use New Poly Fits to Shift Original Flux\n",
    "            original_wavelength_shift = use_new_shifting_factor(wave[index_of_slit],\n",
    "                               no_outlier_wavelength_values_list_shift,left_end_val_shift,\n",
    "                               right_end_val_shift,poly_const_second_deg_shift)\n",
    "        \n",
    "        except TypeError:\n",
    "            poly_const_second_deg_shift = [0,0,0] #no shifting\n",
    "            no_outlier_shifting_values_list = list(shifting_value_dict.values())\n",
    "            no_outlier_wavelength_values_list_shift = weighted_wavelength_list\n",
    "            original_wavelength_shift = wave[index_of_slit]\n",
    "            \n",
    "        #Saving Shifted Original Wavelength as Fits Files \n",
    "        save_new_polynomial_fits_values_shift(no_outlier_shifting_values_list,no_outlier_wavelength_values_list_shift,original_wavelength_shift,mask_name,slit_number_used)\n",
    "    \n",
    "        #Saving Polynomial Coefficients\n",
    "        save_polynomial_coeff(mask_name,slit_number_used,poly_const_first_deg_scal,poly_const_second_deg_shift,left_end_val_scal,right_end_val_scal,no_outlier_wavelength_values_list_scal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slits That Are EXCLUDED From The Median Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looping_finding_scale_excluded_slits(slit_number,index_of_slit,subtraction_dict):\n",
    "\n",
    "    #wavelength_array =  np.arange(4000,11200,200)\n",
    "    wavelength_array = np.arange(4000,11500,500)\n",
    "    #wavelength_array = np.arange(4000, 11350, 350)\n",
    "    \n",
    "    scaling_value_dict = {}\n",
    "\n",
    "    for index in range(len(wavelength_array)):\n",
    "\n",
    "        if (index + 1) == len(wavelength_array): \n",
    "            break\n",
    "\n",
    "        else:\n",
    "        \n",
    "            scaling_value = finding_scaling(slit_number,median, threshold_median,threshold_sky_sub, rbwave_fits[0],wavelength_array[index],\n",
    "                                            wavelength_array[index+1],rbflux_fits_exclude,multipliers,index_of_slit,subtraction_dict)\n",
    "            \n",
    "            scaling_value_dict[\"{0}_to_{1}\".format(wavelength_array[index],wavelength_array[index+1])] = scaling_value\n",
    "            \n",
    "    return scaling_value_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Fits and Weighted Wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automating_weighted_wave_excluded_slits(index_of_slit):\n",
    "    \n",
    "    #wavelength_array =  np.arange(4000,11200,200)\n",
    "    wavelength_array = np.arange(4000,11500,500)\n",
    "    #wavelength_array = np.arange(4000,11350,350)\n",
    "    \n",
    "    weighted_wavelength_list = []\n",
    "\n",
    "    for index in range(len(wavelength_array)):\n",
    "\n",
    "        if (index + 1) == len(wavelength_array): \n",
    "            break\n",
    "\n",
    "        else:\n",
    "        \n",
    "            weighted_wavelength = weighted_wave(median, threshold_median,threshold_sky_sub, rbwave_fits[0],wavelength_array[index],\n",
    "                                          wavelength_array[index+1],rbflux_fits_exclude,index_of_slit)\n",
    "            \n",
    "            weighted_wavelength_list.append(weighted_wavelength)\n",
    "\n",
    "    #weighted_wavelength_list_final = []\n",
    "    \n",
    "    #for wavelength in weighted_wavelength_list: #filter out any None values \n",
    "        \n",
    "        #if wavelength != None:\n",
    "            \n",
    "            #weighted_wavelength_list_final.append(wavelength)\n",
    "        \n",
    "    return weighted_wavelength_list\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate Optimization of Excluded Slits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looping_through_all_function_excluded_slits(slit_numbers_to_automate):\n",
    "    \n",
    "    for slit_number in slit_numbers_to_automate:\n",
    "\n",
    "        slit_number_used = slit_number\n",
    "        print('\\nOptimizing slit #{0} (Excluded) from mask {1}.'.format(slit_number_used,mask_name))\n",
    "\n",
    "        index_of_slit = slit_nums_exclude.index(slit_number_used) #index of slit that are excluded (Contain ISM Em Line)\n",
    "        #slit_nums_exclude will need to be changed for slits that are included\n",
    "\n",
    "        #Determine the Optimal Scale Factor \n",
    "        print('Finding the Optimal Scale Factor...')\n",
    "        subtraction_dict = rbflux_minus_median(rbflux_fits_exclude[index_of_slit], median, multipliers)\n",
    "\n",
    "        scaling_value_dict = looping_finding_scale_excluded_slits(slit_number_used,index_of_slit,subtraction_dict)\n",
    "\n",
    "        #Determine the weighted wavelength \n",
    "        weighted_wavelength_list = automating_weighted_wave_excluded_slits(index_of_slit) #there is some None values. Let's filter it out! \n",
    "        print(\"List of All Weighted Wavelength: {0}\".format(weighted_wavelength_list))\n",
    "        \n",
    "        try:\n",
    "            #Original Polynomial Fits for Scaling\n",
    "            scaling_values_list,wavelength_values_scal,poly_const_first_deg_scaling = wavelength_scaling_function(scaling_value_dict,weighted_wavelength_list)\n",
    "\n",
    "            #Improving Original Polynomial Fits for Scaling\n",
    "            #Remove Outliers Using Sigma Clipping\n",
    "            no_outlier_scaling_values_list, no_outlier_wavelength_values_list_scal = remove_outliers_sigma_clip_scaling(scaling_values_list,wavelength_values_scal,poly_const_first_deg_scaling)\n",
    "\n",
    "            #New Polynomial Fits for Scaling\n",
    "            no_outlier_wavelength_values_list_scal,left_end_val_scal,right_end_val_scal,poly_const_first_deg_scal = new_polynomial_fits(mask_name,slit_number_used,no_outlier_scaling_values_list,no_outlier_wavelength_values_list_scal)\n",
    "\n",
    "            #Use New Poly Fits to Scale Original Flux\n",
    "            original_flux_scale = use_new_scaling_factor(flux_exclude[index_of_slit],wave_exclude[index_of_slit],\n",
    "                               no_outlier_wavelength_values_list_scal,left_end_val_scal,\n",
    "                               right_end_val_scal,poly_const_first_deg_scal)\n",
    "        except TypeError: \n",
    "            poly_const_first_deg_scal = [0,1] #straight horizontal line, no scaling\n",
    "            no_outlier_scaling_values_list = list(scaling_value_dict.values())\n",
    "            no_outlier_wavelength_values_list_scal = weighted_wavelength_list\n",
    "            original_flux_scale = flux_exclude[index_of_slit]\n",
    "            \n",
    "\n",
    "        #Saving Scaled Original Flux as FITS Files\n",
    "        save_new_polynomial_fits_values_scale(no_outlier_scaling_values_list, no_outlier_wavelength_values_list_scal, original_flux_scale,mask_name,slit_number_used)\n",
    "\n",
    "        #Determine the Optimal Shift Factor \n",
    "        print('Finding the Optimal Shift Factor...')\n",
    "        shifting_value_dict,rbflux_shifted_dict = finding_shifting(slit_number_used,flux_exclude,wave_exclude,ivar_exclude,scaling_value_dict,median,threshold_median,rbwave_fits[0],\n",
    "                                            index_of_slit,rbflux_fits_exclude,threshold_sky_sub)\n",
    "\n",
    "        #Saving the Scale Factor and Shift Factor\n",
    "        saving_scaling_and_shifting_factor(mask_name,slit_number_used,scaling_value_dict,shifting_value_dict)\n",
    "        \n",
    "        #Sort through rbflux_shifted_dict and remove all key-value we don't need (reduce size of saved file)\n",
    "        sorted_rbflux_shifted_dict = sorting_needed_shift_factor(shifting_value_dict,rbflux_shifted_dict)\n",
    "        \n",
    "        #Saving the rebinned flux calculated using the shifted wavelength\n",
    "        saving_rbflux_shifted(mask_name,slit_number_used,sorted_rbflux_shifted_dict)\n",
    "\n",
    "        try:\n",
    "            #Original Polynomial Fits for Shifting\n",
    "            shifting_values_list,wavelength_values_shift,poly_const_second_deg_shifting = wavelength_shifting_function(shifting_value_dict,weighted_wavelength_list)\n",
    "\n",
    "            #Improving Original Polynomial Fits for Shifting\n",
    "            #Remove Outliers \n",
    "            no_outlier_shifting_values_list, no_outlier_wavelength_values_list_shift = remove_outliers_sigma_clip_shifting(shifting_values_list,wavelength_values_shift,poly_const_second_deg_shifting)\n",
    "\n",
    "            #New Polynomial Fits for Shifting\n",
    "            no_outlier_wavelength_values_list_shift,left_end_val_shift,right_end_val_shift,poly_const_second_deg_shift = new_polynomial_fits_shifting(mask_name,slit_number_used,no_outlier_shifting_values_list,no_outlier_wavelength_values_list_shift)\n",
    "\n",
    "            #Use New Poly Fits to Shift Original Flux\n",
    "            original_wavelength_shift = use_new_shifting_factor(wave_exclude[index_of_slit],\n",
    "                               no_outlier_wavelength_values_list_shift,left_end_val_shift,\n",
    "                               right_end_val_shift,poly_const_second_deg_shift)\n",
    "        except TypeError:\n",
    "            poly_const_second_deg_shift = [0,0,0] #no shifting\n",
    "            no_outlier_shifting_values_list = list(shifting_value_dict.values())\n",
    "            no_outlier_wavelength_values_list_shift = weighted_wavelength_list\n",
    "            original_wavelength_shift = wave_exclude[index_of_slit]\n",
    "\n",
    "        #Saving Shifted Original Wavelength as Fits Files \n",
    "        save_new_polynomial_fits_values_shift(no_outlier_shifting_values_list, no_outlier_wavelength_values_list_shift, original_wavelength_shift, mask_name,slit_number_used)\n",
    "    \n",
    "        #Saving Polynomial Coefficients\n",
    "        save_polynomial_coeff(mask_name,slit_number_used,poly_const_first_deg_scal,poly_const_second_deg_shift,left_end_val_scal,right_end_val_scal,no_outlier_wavelength_values_list_scal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code used to generate the log files during optimization\n",
    "class multifile(object):\n",
    "    def __init__(self, files):\n",
    "        self._files = files\n",
    "    def __getattr__(self, attr, *args):\n",
    "        return self._wrap(attr, *args)\n",
    "    def _wrap(self, attr, *args):\n",
    "        def g(*a, **kw):\n",
    "            for f in self._files:\n",
    "                res = getattr(f, attr, *args)(*a, **kw)\n",
    "            return res\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through all relevant functions to optimize included slits\n",
    "\n",
    "slit_numbers_you_want_to_run_included_slits = slit_nums #put in the slit numbers you want to run \n",
    "\n",
    "# The text output is written to a log file.\n",
    "# The log file is overwritten (not appended) every time this cell is executed.\n",
    "# If you want to keep a record of particular run, make sure you save a copy of the file before running this cell again.\n",
    "try:\n",
    "    log_file_inc = open('./{0}_Polynomial_Graph/{0}_Included_Optimization.log'.format(mask_name), 'w')\n",
    "    sys.stdout = multifile([ std_out, log_file_inc ])\n",
    "    sys.stderr = multifile([ std_err, log_file_inc ])\n",
    "    looping_through_all_function_included_slits(slit_numbers_you_want_to_run_included_slits)\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    log_file_inc.close()\n",
    "    sys.stdout = std_out\n",
    "    sys.stderr = std_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through all relevant functions to optimize included slits\n",
    "\n",
    "slit_numbers_you_want_to_run_excluded_slits = [93, 95, 97, 99, 100, 101, 102, 108, 109, 110, 112, 113, 116, 117, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 140, 142, 143, 144, 146, 147, 148, 149, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 164, 165, 166, 167, 169, 172, 176, 177, 185, 186, 187, 191, 192, 193, 195, 199] #put the slit number of all slits you want to iterate\n",
    "\n",
    "# The text output is written to a log file.\n",
    "# The log file is overwritten (not appended) every time this cell is executed.\n",
    "# If you want to keep a record of particular run, make sure you save a copy of the file before running this cell again.\n",
    "try:\n",
    "    log_file_exc = open('./{0}_Polynomial_Graph/{0}_Excluded_Optimization.log'.format(mask_name), 'w')\n",
    "    sys.stdout = multifile([ std_out, log_file_exc ])\n",
    "    sys.stderr = multifile([ std_err, log_file_exc ])\n",
    "    looping_through_all_function_excluded_slits(slit_numbers_you_want_to_run_excluded_slits)\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    log_file_exc.close()\n",
    "    sys.stdout = std_out\n",
    "    sys.stderr = std_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Back Scale and Shift Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_scale_and_shift_factor(mask_name,slit_number_used):\n",
    "    \n",
    "    scale_and_shift_factor_hdu = fits.open(\"{0}_Rebinned/{0}_Scaling_and_Shifting_Factor/Scaling_and_Shifting_Factor_{0}_{1}.fits.gz\".format(mask_name,slit_number_used))\n",
    "    scale_factor = scale_and_shift_factor_hdu[1].data[\"SCALING_FACTOR\"]\n",
    "    shift_factor = scale_and_shift_factor_hdu[1].data[\"SHIFTING_FACTOR\"]\n",
    "            \n",
    "    return scale_factor,shift_factor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rounding Scale and Shift Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounding_the_factors(scale_factor_list,shift_factor_list):\n",
    "    \n",
    "    rounded_scale_factor_list = []\n",
    "    for value in scale_factor_list:\n",
    "        rounded_scale_factor_list.append(round(value * 1,2))\n",
    "\n",
    "    rounded_shift_factor_list = []\n",
    "    for value in shift_factor_list:\n",
    "        rounded_shift_factor_list.append(round(value * 1,2))\n",
    "\n",
    "    return rounded_scale_factor_list, rounded_shift_factor_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Back Rebinned Flux w/ Shifted Wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rbflux_shifted_wave(mask_name,slit_number_used):\n",
    "    read_dict = np.load(\"./{0}_Rebinned/{0}_Rebinned_Flux_Shifted_Wave/rbflux_shifted_dict_{0}_{1}.npy\".format(mask_name,slit_number_used),allow_pickle=True).item()\n",
    "    return read_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Back Polynomial Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_poly_coeff(mask_name,slit_number_used):\n",
    "    \n",
    "    #get the polynomial coefficients and stores them\n",
    "    \n",
    "    poly_coeff = fits.open(\"{0}_Rebinned/{0}_Polynomial_Coefficients/Poly_Coeff_{0}_{1}.fits.gz\".format(mask_name,slit_number_used))\n",
    "    \n",
    "    read_poly_coeff_scale = poly_coeff[1].data[\"POLYNOMIAL_COEFFICIENTS_SCALING\"]\n",
    "    \n",
    "    read_poly_coeff_shift = poly_coeff[1].data[\"POLYNOMIAL_COEFFICIENTS_SHIFTING\"]\n",
    "    \n",
    "    end_values = poly_coeff[1].data[\"END_VALUES\"]\n",
    "    \n",
    "    no_outlier_weighted_wave = poly_coeff[1].data[\"NO_OUTLIER_WEIGHTED_WAVE\"]\n",
    "    \n",
    "    return read_poly_coeff_scale,read_poly_coeff_shift,end_values,no_outlier_weighted_wave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Optimal Shift Factor and Optimal Scale Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pranav's function use Optimal Scaling Factor and Optimal Shifting Factor to plot \n",
    "def plotting_with_shifting_all(index_of_slit,median,rbflux_shifted_dict,rebinned_flux,shifting_values,poly_coeff_scale,end_values,no_outlier_weighted_wave,min_ylim,max_ylim):\n",
    "    \n",
    "    wavelength_array = np.arange(4000,11500,500)\n",
    "    \n",
    "    new_flux_joined = []\n",
    "\n",
    "    for index in range(len(wavelength_array)):\n",
    "        \n",
    "        if (index + 1) == len(wavelength_array): \n",
    "            break\n",
    "        \n",
    "        else: \n",
    "        \n",
    "            shifting_factor = shifting_values #list of shifting factors\n",
    "            \n",
    "            spectrum = rebinned_flux[index_of_slit] #original rebinned flux \n",
    "\n",
    "            #For segments that DOES NOT have Opt Shifting Factor\n",
    "            if np.isfinite(shifting_values[index]) == False: \n",
    "                #new_flux_no_scaling.append(spectrum) \n",
    "                fx = np.array(spectrum)\n",
    "                l = list(fx[np.where((new_wave_600>=wavelength_array[index]) & (new_wave_600<wavelength_array[index+1]))])\n",
    "                new_flux_joined = new_flux_joined + l\n",
    "                    \n",
    "                            \n",
    "            #For segments that do have Opt Scaling Factor and Opt Shifting Factor\n",
    "            else: \n",
    "                spectrum_shifted = rbflux_shifted_dict[\"Shifted_{0}\".format(shifting_values[index],2)][0]\n",
    "                #new_flux_no_scaling.append(spectrum_shifted)\n",
    "                fx = np.array(spectrum_shifted)\n",
    "                l = list(fx[np.where((new_wave_600>=wavelength_array[index]) & (new_wave_600<wavelength_array[index+1]))])\n",
    "                new_flux_joined = new_flux_joined + l\n",
    "    \n",
    "    all_scale_factors = [] #calculate all the scale factor using polynomial function\n",
    "    \n",
    "    for wavelength in new_wave_600:\n",
    "        if wavelength < min(no_outlier_weighted_wave):\n",
    "            all_scale_factors.append(end_values[0])\n",
    "             \n",
    "        elif wavelength > max(no_outlier_weighted_wave):\n",
    "            all_scale_factors.append(end_values[1])\n",
    "        \n",
    "        else:\n",
    "            scaling_factor = polynomial_first_order(wavelength,poly_coeff_scale)\n",
    "            all_scale_factors.append(scaling_factor)\n",
    "    \n",
    "    new_rbflux_scaled = (np.array(new_flux_joined) * np.array(all_scale_factors)) - np.array(median)\n",
    "    \n",
    "    return new_rbflux_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making A New Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_flux_and_wavelength(slit_nums,mask_name):\n",
    "    \n",
    "    scaled_original_flux_fits = []\n",
    "    shifted_original_wavelength_fits = []\n",
    "    \n",
    "    for slit_num in slit_nums:\n",
    "        \n",
    "        scaled_flux = fits.open(\"{0}_Rebinned/{0}_Scale_Values/Scale_Values_Polynomial_Fits_{0}_{1}.fits.gz\".format(mask_name,slit_num))\n",
    "        shifted_wavelength = fits.open((\"{0}_Rebinned/{0}_Shift_Values/Shift_Values_Polynomial_Fits_{0}_{1}.fits.gz\".format(mask_name,slit_num)))\n",
    "        scaled_original_flux_fits.append(scaled_flux[1].data[\"ORIGINAL_FLUX_SCALED\"])\n",
    "        shifted_original_wavelength_fits.append(shifted_wavelength[1].data[\"ORIGINAL_WAVELENGTH_SHIFTED\"])\n",
    "    \n",
    "    return scaled_original_flux_fits, shifted_original_wavelength_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_median(rebinned_flux_array):\n",
    "    \n",
    "    median_vals = []\n",
    "    \n",
    "    print(len(rebinned_flux_array))\n",
    "    \n",
    "    for i in range(len(rebinned_flux_array[0])):\n",
    "\n",
    "        comp = []\n",
    "        \n",
    "        for array in rebinned_flux_array:\n",
    "            \n",
    "            if np.isfinite(array[i]) == True:\n",
    "                comp.append(array[i])\n",
    "                \n",
    "        median_vals.append(np.median(comp))\n",
    "        \n",
    "    return median_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_original_flux_fits,shifted_original_wavelength_fits = read_flux_and_wavelength(slit_nums,mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rbflux,new_rbwavelength,new_rbivar = rebin(scaled_original_flux_fits,shifted_original_wavelength_fits,ivar,600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_median = find_median(new_rbflux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automation of Excluded Slits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automation_func_exclude(mask_name,slit_number):\n",
    "    \n",
    "    #slit number\n",
    "    slit_number_used = slit_number \n",
    "    \n",
    "    #index of slit number using list of excluded slits \n",
    "    index_of_slit = slit_nums_exclude.index(slit_number_used)\n",
    "    \n",
    "    #get scale factor and shift factor from FITS files\n",
    "    scale_factor_list,shift_factor_list = read_scale_and_shift_factor(mask_name,slit_number_used)\n",
    "    \n",
    "    #Display whole number, but is actually a float \n",
    "    #Round it to whole number \n",
    "    rounded_scale_factor_list,rounded_shift_factor_list = rounding_the_factors(scale_factor_list,shift_factor_list)\n",
    "    \n",
    "    #Rebinned flux w/ shifted wavelength \n",
    "    rbflux_shifted_dict = read_rbflux_shifted_wave(mask_name,slit_number_used)\n",
    "    \n",
    "    #Read poly coeff\n",
    "    poly_coeff_scale,poly_coeff_shift,end_values,no_outlier_weighted_wave = read_poly_coeff(mask_name,slit_number)\n",
    "    \n",
    "    #subtraction using new median\n",
    "    new_flux = plotting_with_shifting_all(index_of_slit,new_median,rbflux_shifted_dict,rbflux_fits_exclude,rounded_shift_factor_list,poly_coeff_scale,end_values,no_outlier_weighted_wave,-500,1000)\n",
    "    \n",
    "    return new_flux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combining_all_exclude_new_flux(mask_name,slit_nums_exclude):\n",
    "    new_flux_list = []\n",
    "\n",
    "    for slit_number in slit_nums_exclude:\n",
    "\n",
    "        new_flux = automation_func_exclude(mask_name,slit_number)\n",
    "\n",
    "        new_flux_list.append(new_flux)\n",
    "        \n",
    "    return new_flux_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_flux_list_exclude_slits = combining_all_exclude_new_flux(mask_name,slit_nums_exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automation of Included Slits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automation_func_include(mask_name,slit_number):\n",
    "    \n",
    "    #slit number\n",
    "    slit_number_used = slit_number \n",
    "    \n",
    "    #index of slit number using list of excluded slits \n",
    "    index_of_slit = slit_nums.index(slit_number_used)\n",
    "    \n",
    "    #get scale factor and shift factor from FITS files\n",
    "    scale_factor_list,shift_factor_list = read_scale_and_shift_factor(mask_name,slit_number_used)\n",
    "    \n",
    "    #Display whole number, but is actually a float \n",
    "    #Round it to whole number \n",
    "    rounded_scale_factor_list,rounded_shift_factor_list = rounding_the_factors(scale_factor_list,shift_factor_list)\n",
    "    \n",
    "    #Rebinned flux w/ shifted wavelength \n",
    "    rbflux_shifted_dict = read_rbflux_shifted_wave(mask_name,slit_number_used)\n",
    "    \n",
    "    #Read poly coeff\n",
    "    poly_coeff_scale,poly_coeff_shift,end_values,no_outlier_weighted_wave = read_poly_coeff(mask_name,slit_number)\n",
    "    \n",
    "    #subtraction using new median\n",
    "    new_flux = plotting_with_shifting_all(index_of_slit,new_median,rbflux_shifted_dict,rbflux_fits,rounded_shift_factor_list,poly_coeff_scale,end_values,no_outlier_weighted_wave,-500,1000)\n",
    "    \n",
    "    return new_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combining_all_include_new_flux(mask_name,slit_nums):\n",
    "    new_flux_list = []\n",
    "\n",
    "    for slit_number in slit_nums:\n",
    "\n",
    "        new_flux = automation_func_include(mask_name,slit_number)\n",
    "\n",
    "        new_flux_list.append(new_flux)\n",
    "        \n",
    "    return new_flux_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_flux_list_include_slits = combining_all_include_new_flux(mask_name,slit_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Optimized Spectrum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_opt_spectrum(opt_spectrum,slit_number,incl_or_excl): #save optimized and trimmed spectrum\n",
    "    hdu1 = fits.PrimaryHDU()\n",
    "    \n",
    "    c1 = fits.Column(name=\"OPTIMIZED_SPECTRUM\", array=opt_spectrum, format=\"E\")\n",
    "    \n",
    "    hdu2 = fits.BinTableHDU.from_columns([c1])\n",
    "    \n",
    "    hdul = fits.HDUList([hdu1,hdu2])\n",
    "    \n",
    "    hdul.writeto(\"{0}_Trimmed_Spectra/{0}/Optimized_Spectrum_{1}_{2}.fits.gz\".format(incl_or_excl,mask_name,slit_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Instrumental Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_rms(a, window=50, reverse=False):\n",
    "    \n",
    "    '''\n",
    "    Returns the moving RMS values up to the halfway point of the array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : ndarray\n",
    "        One dimensional flux array.\n",
    "    window : int, optional\n",
    "        The size of each segment for taking the RMS.\n",
    "    reverse : bool, optional\n",
    "        Can be used to switch between left-to-right\n",
    "        and right-to-left directions for the moving RMS.\n",
    "        Default is left-to-right.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    rms_arr : One dimensional array of moving RMS.\n",
    "    '''\n",
    "    \n",
    "    rms_arr = []\n",
    "    b = np.where(np.isfinite(a))[0]\n",
    "    mid_index = int((b[-1]+b[0])/2)\n",
    "    \n",
    "    if reverse==False:\n",
    "        p,q,r = 0,mid_index,1\n",
    "    else:\n",
    "        p,q,r = -1,-(len(a)-mid_index-1),-1\n",
    "    \n",
    "    for i in range(p,q,r):\n",
    "        if (reverse==False):\n",
    "            x = a[i:i+window]\n",
    "        else:\n",
    "            x = a[i:i-window:-1]\n",
    "            \n",
    "        rms = np.sqrt(np.mean(x**2))\n",
    "        \n",
    "        if (reverse==False):\n",
    "            rms_arr.append(rms)\n",
    "        else:\n",
    "            rms_arr.insert(0,rms)\n",
    "\n",
    "    return np.array(rms_arr)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_median(a, window=50, reverse=False):\n",
    "    \n",
    "    '''\n",
    "    Returns the moving median values up to the halfway point of the array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : ndarray\n",
    "        One dimensional flux array.\n",
    "    window : int, optional\n",
    "        The size of each segment for taking the median.\n",
    "    reverse : bool, optional\n",
    "        Can be used to switch between left-to-right\n",
    "        and right-to-left directions for the moving median.\n",
    "        Default is left-to-right.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    median_arr : One dimensional array of moving RMS.\n",
    "    '''\n",
    "    \n",
    "    median_arr = []\n",
    "    b = np.where(np.isfinite(a))[0]\n",
    "    mid_index = int((b[-1]+b[0])/2)\n",
    "    \n",
    "    if reverse==False:\n",
    "        p,q,r = 0,mid_index,1\n",
    "    else:\n",
    "        p,q,r = -1,-(len(a)-mid_index-1),-1\n",
    "    \n",
    "    for i in range(p,q,r):\n",
    "        if (reverse==False):\n",
    "            x = a[i:i+window]\n",
    "        else:\n",
    "            x = a[i:i-window:-1]\n",
    "        if np.isfinite(a[i]):\n",
    "            median = np.median(x[np.where(np.isfinite(x))[0]])\n",
    "        else:\n",
    "            median = np.nan\n",
    "        \n",
    "        if (reverse==False):\n",
    "            median_arr.append(median)\n",
    "        else:\n",
    "            median_arr.insert(0,median)\n",
    "    \n",
    "    return np.array(median_arr)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v 11.2\n",
    "\n",
    "def end_trimming_auto(rbwave, rbflux, left_limit, right_limit, minvalue_l=30, minvalue_r=30, window_l=50, window_r=50, GetBinArray=False):\n",
    "    \n",
    "    '''\n",
    "    \\nTrims the instrumental errors at the extreme left and right ends\n",
    "    of a flux array, using a moving RMS function. Pads the ends with\n",
    "    NaN to preserve the length of the array.\n",
    "    \n",
    "    Version - 11.2\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rbwave : ndarray\n",
    "        One dimensional (rebinned) wave array\n",
    "    rbflux : ndarray\n",
    "        One dimensional (rebinned) flux array.\n",
    "    left_limit : int or float, optional\n",
    "        Size of segment at the left end, within which to look for\n",
    "        the instrumental effects. The value passed should be in terms\n",
    "        of wavelength. The function will only look within a range\n",
    "        of left_limit angstroms, and not any further.\n",
    "    right_limit : int or float, optional\n",
    "        Size of segment at the right end, within which to look for\n",
    "        the instrumental effects. The value passed should be in terms\n",
    "        of wavelength. The function will only look within a range\n",
    "        of right_limit angstroms, and not any further.\n",
    "    minvalue_l : int or float, optional\n",
    "        Cut-off value at the left end. Flux array is trimmed only when\n",
    "        the moving RMS value goes below this value at the left end.\n",
    "    minvalue_r : int or float, optional\n",
    "        Cut-off value at the right end. Flux array is trimmed when\n",
    "        the moving RMS value goes below this value at the right end.\n",
    "    window_l : int or float, optional\n",
    "        The size of each segment for taking the left-to-right RMS.\n",
    "    window_r : int or float, optional\n",
    "        The size of each segment for taking the right-to-left RMS.\n",
    "    GetBinArray : bool, optional\n",
    "        Can return the boolean array of indices of the original flux\n",
    "        array retained after trimming.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    trmdfx : ndaray\n",
    "        One dimensional array of trimmed flux.\n",
    "    bin_array : ndarray, only returned if GetBinArray is set to True\n",
    "        Boolean array of indices retained from original flux array.\n",
    "        \n",
    "    Notes\n",
    "    ----------\n",
    "    Uses the moving_rms() and moving_median() functions to operate.\n",
    "    '''\n",
    "    \n",
    "    a = np.where(np.isfinite(rbflux))[0] #Fiding the indices of finite flux values\n",
    "    mid_index = int((a[-1]+a[0])/2)\n",
    "    b = rbwave[np.isfinite(rbflux)]\n",
    "    b_l = np.where(rbwave<(b[0]+left_limit))[0]\n",
    "    left_limit_index = b_l[-1]\n",
    "    b_r = np.where(rbwave>(b[-1]-right_limit))[0]\n",
    "    right_limit_index = b_r[0] - len(rbwave)\n",
    "    \n",
    "    rms_l = moving_rms(rbflux,window=window_l)\n",
    "    median_l = np.concatenate((moving_median(rbflux,window=200),np.zeros(len(rbflux)-mid_index)))\n",
    "    flux_around_median_l = rbflux - median_l\n",
    "    local_rms_l = moving_rms(flux_around_median_l,window=5)\n",
    "    \n",
    "    rms_r = moving_rms(rbflux,window=window_r,reverse=True)\n",
    "    median_r = np.concatenate((np.zeros(mid_index+2),moving_median(rbflux,window=200,reverse=True)))\n",
    "    flux_around_median_r = rbflux - median_r\n",
    "    local_rms_r = moving_rms(flux_around_median_r,window=5,reverse=True)\n",
    "    \n",
    "    trmdfx = np.empty_like(rbflux,dtype=float)\n",
    "    trmdfx[:] = rbflux\n",
    "    \n",
    "    #Trimming for the left end...\n",
    "    for i in range(1,left_limit_index): #Start from 1. Starting from 0 would give IndexError for slope_b\n",
    "        slope_b = rms_l[i-1]-rms_l[i] #RMS slope b/w this point & previous one. No need to divide by rbwave difference.\n",
    "        if (rms_l[i]<minvalue_l):\n",
    "            if (True not in np.isnan(rbflux[i:mid_index])): #Make sure there are no gaps further down\n",
    "                if (slope_b>0): #Is slope dropping towards mid_index?\n",
    "                    break\n",
    "                elif (np.isnan(slope_b)): #Is there NaN before this point? Is this where the finite part begins?\n",
    "                    break\n",
    "    \n",
    "    for k in range(i,left_limit_index):\n",
    "        local_slope_b = local_rms_l[i-1]-rms_l[i]\n",
    "        if (np.abs(flux_around_median_l[k])<minvalue_l and local_rms_l[k]<minvalue_l):\n",
    "            i = k+1\n",
    "            break\n",
    "    \n",
    "    #Trimmming for the right end...\n",
    "    for j in range(-2,right_limit_index,-1): #Start from -2. Starting from -1 would give IndexError for slope_b\n",
    "        slope_b = rms_r[j+1]-rms_r[j] #RMS slope b/w this point & previous one. No need to divide by rbwave difference.\n",
    "        \n",
    "        if (rms_r[j]<minvalue_r):\n",
    "            if (True not in np.isnan(rbflux[mid_index:j])): #Make sure there are no gaps further down\n",
    "                if (slope_b>0): #Is slope dropping towards mid_index?\n",
    "                    break\n",
    "                elif (np.isnan(slope_b)): #Is there NaN before this point? Is this where the finite part ends?\n",
    "                    break\n",
    "\n",
    "    for k in range(j,right_limit_index,-1):\n",
    "        if (np.abs(flux_around_median_r[k])<minvalue_r and local_rms_r[k]<minvalue_r):\n",
    "            j = k-1\n",
    "            break                    \n",
    "\n",
    "    bin_array = np.array([True]*len(rbflux))\n",
    "    bin_array[i:j+1] = False\n",
    "    trmdfx[bin_array] = np.nan\n",
    "    \n",
    "    if GetBinArray==True:\n",
    "        return(np.array(trmdfx),np.invert(bin_array))\n",
    "    else:\n",
    "        return(np.array(trmdfx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minvalue_l = 30 ; minvalue_r = 50\n",
    "window_l = 100 ; window_r = 150\n",
    "left_limit = 1000 ; right_limit = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "included_fluxes_dict = {}; included_slits_opt_trimmed_flux_dict = {}\n",
    "for slit_number in slit_nums:\n",
    "    new_fx = automation_func_include(mask_name,slit_number)\n",
    "    included_fluxes_dict[slit_number] = new_fx\n",
    "    new_fx_trmd,b = end_trimming_auto(rbwave,new_fx,\n",
    "                                      left_limit,right_limit,\n",
    "                                      minvalue_l,minvalue_r,\n",
    "                                      window_l,window_r,\n",
    "                                      GetBinArray=True)\n",
    "    included_slits_opt_trimmed_flux_dict[slit_number] = (new_fx_trmd,b)\n",
    "    print('Done with slit #',slit_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "excluded_fluxes_dict = {}; excluded_slits_opt_trimmed_flux_dict = {}\n",
    "for slit_number in slit_nums_exclude:\n",
    "    new_fx = automation_func_exclude(mask_name,slit_number)\n",
    "    excluded_fluxes_dict[slit_number] = new_fx\n",
    "    new_fx_trmd,b = end_trimming_auto(rbwave,new_fx,\n",
    "                                      left_limit,right_limit,\n",
    "                                      minvalue_l,minvalue_r,\n",
    "                                      window_l,window_r,\n",
    "                                      GetBinArray=True)\n",
    "    excluded_slits_opt_trimmed_flux_dict[slit_number] = (new_fx_trmd,b)\n",
    "    print('Done with slit #',slit_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_plots(slits,excl_or_incl):\n",
    "\n",
    "#     rbwave = new_wave_600\n",
    "    \n",
    "    if excl_or_incl == 'Excluded':\n",
    "        opt_flux_dict = excluded_fluxes_dict\n",
    "        trmd_flux_dict = excluded_slits_opt_trimmed_flux_dict\n",
    "    elif excl_or_incl == 'Included':\n",
    "        opt_flux_dict = included_fluxes_dict\n",
    "        trmd_flux_dict = included_slits_opt_trimmed_flux_dict\n",
    "\n",
    "    path = './Test Figures/{0}/{1}/'.format(mask_name,excl_or_incl)\n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise\n",
    "    \n",
    "    notes_path = \"./Raja's notes/{0}_notes.txt\".format(mask_name)        \n",
    "    try:\n",
    "        with open(notes_path,'r') as f:\n",
    "            lines = [x.split() for x in f.readlines()]\n",
    "            notes = {int(x[0]) : ' '.join(x[3:]) for x in lines}\n",
    "            print_notes = True\n",
    "    except IOError:\n",
    "        print_notes = False\n",
    "    \n",
    "    plt.ioff()\n",
    "    for i in slits:\n",
    "        fig = plt.figure(figsize=(8,6),dpi=600)\n",
    "        fig.patch.set_alpha(1)\n",
    "        \n",
    "        y = opt_flux_dict[i]\n",
    "        y1,b = trmd_flux_dict[i]\n",
    "        y2 = np.empty_like(y)\n",
    "        y2[:] = y; y2[b] = np.nan\n",
    "#         y5 = moving_median(y,window=200)\n",
    "#         y6 = moving_median(y,window=200,reverse=True)\n",
    "#         y3_ = y-np.concatenate((y5,np.array([np.nan]),np.zeros(len(y)-len(y5)-1)))\n",
    "#         y3 = moving_rms(y,window=window_l)\n",
    "#         y4_ = y-np.concatenate((np.zeros(len(y)-len(y6)-1),np.array([np.nan]),y6))\n",
    "#         y4 = moving_rms(y,window=window_r,reverse=True)\n",
    "#         y8 = np.median(y[np.isfinite(y)])*np.ones(len(y))[b]\n",
    "\n",
    "        plt.plot(rbwave,y1,'dodgerblue',ls='-',label='Part retained')\n",
    "        plt.plot(rbwave,y2,'red',ls='-',label='Part trimmed')\n",
    "#         plt.plot(rbwave[0:len(y5)],y5,'limegreen',ls=':',lw=0.6,label='Moving median (L-R)')\n",
    "#         plt.plot(rbwave[-len(y6)-1:-1],y6,'brown',ls=':',lw=0.6,label='Moving median (R-L)')\n",
    "#         plt.plot(rbwave[0:len(y3)],y3,'blue',ls='-',lw=0.8,label='Moving RMS (L-R)')\n",
    "#         plt.plot(rbwave[-len(y4)-1:-1],y4,'gold',ls='-',lw=0.8,label='Moving RMS (R-L)')\n",
    "#         plt.plot(rbwave[b],y8,'darkorange',ls=':',lw=0.5,label='Median of whole flux array')\n",
    "        plt.xlabel(\"Wavelength (A)\")\n",
    "        plt.ylabel(\"Flux (Electron/Hour)\")\n",
    "#         plt.title('Trimming function version 11.2')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.suptitle('Test figure: slit #{0} in mask {1} ({2})'.format(i,mask_name,excl_or_incl),fontsize=18)\n",
    "        if (print_notes == True):\n",
    "            txt = 'Notes: {0}'.format(notes[i])\n",
    "            plt.figtext(0.95, 0.08, txt, alpha=0.5, wrap=True, ha='right', va='top', fontstyle='italic', fontsize=8)\n",
    "            plt.tight_layout(rect=[0,0.1,0.9,1])\n",
    "        s = 'test_figure_{0}_slit_{1}.png'.format(mask_name,i)\n",
    "        plt.savefig(path+s)\n",
    "        print('Saved figure {0}'.format(s))\n",
    "        plt.close('all')\n",
    "    plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_plots(slit_nums,\"Included\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_plots(slit_nums_exclude,\"Excluded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Optimized Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_optimized_data(mask_name, file_names):\n",
    "        \n",
    "    spectrum = []\n",
    "        \n",
    "    for slit in file_names: \n",
    "        spectrum_data_slit = fits.open(path_name + \"/{0}_Trimmed_Spectra/Optimized_Spectrum_Flux/Optimized_Spectrum_{0}_{1}.fits.gz\".format(mask_name,slit))\n",
    "        spectrum.append(spectrum_data_slit[1].data[\"OPTIMIZED_SPECTRUM\"])\n",
    "            \n",
    "    return spectrum   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot one slit with emission lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emission_lines():\n",
    "    filepath = 'optical_linelist.dat.txt'\n",
    "    fp = open(filepath)\n",
    "    all_emission_lines = []\n",
    "    is_first_line = True\n",
    "    for line in (fp):\n",
    "        emission_line = line.split()[1]\n",
    "        all_emission_lines.append(emission_line)\n",
    "    \n",
    "    \n",
    "    all_emission_lines.pop(0)\n",
    "    \n",
    "    \n",
    "    return all_emission_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_emission_lines(slit_number, min_ylim=-300, max_ylim=700, min_xlim=4000, max_xlim=11000):\n",
    "    \n",
    "    spectrum = read_optimized_data(mask_name, all_slit_nums)\n",
    "    \n",
    "    slit_index = all_slit_nums.index(slit_number)\n",
    "    \n",
    "    all_emission_lines = emission_lines()\n",
    "    \n",
    "    plt.ylim(min_ylim,max_ylim) \n",
    "    plt.xlim(min_xlim,max_xlim)\n",
    "    plt.xlabel(\"Wavelength (A)\")\n",
    "    plt.ylabel(\"Flux (Electron/Hour)\")\n",
    "    plt.title(\"Slit {0} ({1} A to {2} A)\".format(slit_number, min_xlim, max_xlim))\n",
    "    \n",
    "    plt.plot(rbwave,spectrum[slit_index])\n",
    "    for line in all_emission_lines:\n",
    "        plt.axvline(float(line), min_ylim, max_ylim, c = \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_with_emission_lines(5, -300, 700, 6200, 6400)\n",
    "plot_with_emission_lines()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
